<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Scrapy框架 - WebDog</title><meta description="安装安装scrapy 1pip install scrapy  安装pypiwin32(Windows系统) 1pip install pypiwin32      123456789101112131415161.引擎(Scrapy Engine)用来处理整个系统的数据流处理, 触发事务(框架核心)2.调度器(Scheduler)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返"><meta property="og:type" content="blog"><meta property="og:title" content="Scrapy框架"><meta property="og:url" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/"><meta property="og:site_name" content="WebDog"><meta property="og:description" content="安装安装scrapy 1pip install scrapy  安装pypiwin32(Windows系统) 1pip install pypiwin32      123456789101112131415161.引擎(Scrapy Engine)用来处理整个系统的数据流处理, 触发事务(框架核心)2.调度器(Scheduler)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/1.png"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/2.png"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/3.png"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/4.png"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/5.png"><meta property="og:image" content="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/6.png"><meta property="article:published_time" content="2020-02-15T11:15:24.000Z"><meta property="article:modified_time" content="2020-04-07T16:01:13.841Z"><meta property="article:author" content="WebDog"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/Scrapy%E6%A1%86%E6%9E%B6/1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/"},"headline":"WebDog","image":["http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/1.png","http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/2.png","http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/3.png","http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/4.png","http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/5.png","http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/6.png"],"datePublished":"2020-02-15T11:15:24.000Z","dateModified":"2020-04-07T16:01:13.841Z","author":{"@type":"Person","name":"WebDog"},"description":"安装安装scrapy 1pip install scrapy  安装pypiwin32(Windows系统) 1pip install pypiwin32      123456789101112131415161.引擎(Scrapy Engine)用来处理整个系统的数据流处理, 触发事务(框架核心)2.调度器(Scheduler)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返"}</script><link rel="canonical" href="http://yoursite.com/Scrapy%E6%A1%86%E6%9E%B6/"><link rel="alternative" href="/atom.xml" title="WebDog" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="WebDog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/webysx"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-02-15T11:15:24.000Z" title="2020-02-15T11:15:24.000Z">2020-02-15</time><span class="level-item">18 分钟 读完 (大约 2682 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">Scrapy框架</h1><div class="content"><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p><strong>安装scrapy</strong></p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> scrapy</span><br></pre></td></tr></table></figure>

<p><strong>安装pypiwin32(Windows系统)</strong></p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> pypiwin32</span><br></pre></td></tr></table></figure>



<img src="/Scrapy%E6%A1%86%E6%9E%B6/1.png" alt="1" style="zoom: 150%;">

<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>引擎<span class="comment">(Scrapy Engine)</span></span><br><span class="line">用来处理整个系统的数据流处理, 触发事务<span class="comment">(框架核心)</span></span><br><span class="line"><span class="number">2.</span>调度器<span class="comment">(Scheduler)</span></span><br><span class="line">用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回；由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</span><br><span class="line"><span class="number">3.</span>下载器<span class="comment">(Downloader)</span></span><br><span class="line">用于下载网页内容, 并将网页内容返回给蜘蛛<span class="comment">(Scrapy下载器是建立在twisted这个高效的异步模型上的)</span></span><br><span class="line"><span class="number">4.</span>爬虫<span class="comment">(Spiders)</span></span><br><span class="line">爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体<span class="comment">(Item)</span>。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</span><br><span class="line"><span class="number">5.</span>项目管道<span class="comment">(Pipeline)</span></span><br><span class="line">负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息</span><br><span class="line"><span class="number">6.</span>下载器中间件<span class="comment">(Downloader Middlewares)</span></span><br><span class="line">位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应</span><br><span class="line"><span class="number">7.</span>爬虫中间件<span class="comment">(Spider Middlewares)</span></span><br><span class="line">介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出</span><br><span class="line"><span class="number">8.</span>调度中间件<span class="comment">(Scheduler Middewares)</span></span><br><span class="line">介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应</span><br></pre></td></tr></table></figure>



<h1 id="环境搭建（以爬取糗事百科为例）"><a href="#环境搭建（以爬取糗事百科为例）" class="headerlink" title="环境搭建（以爬取糗事百科为例）"></a>环境搭建（以爬取糗事百科为例）</h1><p><strong>创建项目（myspider）</strong></p>
<p>使用scrapy框架创建项目，需要通过命令行来创建。首先进入项目所在目录，然后打开命令行，输入如下命令：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">scrapy</span> startproject<span class="meta"> [项目名称]</span></span><br></pre></td></tr></table></figure>

<p><img src="/Scrapy%E6%A1%86%E6%9E%B6/2.png" alt="2"></p>
<p><img src="/Scrapy%E6%A1%86%E6%9E%B6/3.png" alt="3"></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1<span class="selector-class">.items</span><span class="selector-class">.py</span>：用来存放爬虫爬取下来的数据的模型</span><br><span class="line">2<span class="selector-class">.middlewares</span><span class="selector-class">.py</span>：用来存放各种中间件的文件</span><br><span class="line">3<span class="selector-class">.pipelines</span><span class="selector-class">.py</span>：用来将<span class="selector-tag">items</span>的模型存储到本地磁盘中</span><br><span class="line">4<span class="selector-class">.setting</span><span class="selector-class">.py</span>：本爬虫的一些配置信息（比如请求头，设置延时，<span class="selector-tag">ip</span>代理池等）</span><br><span class="line">5<span class="selector-class">.scrapy</span><span class="selector-class">.cfg</span>：项目的配置文件</span><br><span class="line">6<span class="selector-class">.spider</span>包：所有的爬虫，都是存放在这个文件夹里面</span><br></pre></td></tr></table></figure>

<p><strong>创建爬虫（qsbk.py）</strong></p>
<p>先进入到项目文件夹中，然后执行如下命令，创建爬虫文件：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scrapy </span>genspider qsbk <span class="string">"qiushibaike.com"</span></span><br></pre></td></tr></table></figure>

<p><img src="/Scrapy%E6%A1%86%E6%9E%B6/4.png" alt="4"></p>
<p>爬虫名称不能和项目名称相同；</p>
<p><strong>运行爬虫（qsbk.py）</strong></p>
<p>先进入到项目文件夹中，然后执行如下命令：</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">scrapy crawl qsbk</span></span><br></pre></td></tr></table></figure>

<p><img src="/Scrapy%E6%A1%86%E6%9E%B6/5.png" alt="5"></p>
<h1 id="爬虫代码"><a href="#爬虫代码" class="headerlink" title="爬虫代码"></a>爬虫代码</h1><ol>
<li><p>设置settings.py文件；</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">   <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">   <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果不想每次都在命令行中运行爬虫，可以在爬虫myspider跟目录下创建一个运行文件<code>run.py</code>，代码如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line"># cmdline.<span class="keyword">execute</span>("scrapy crawl qsbk".split()) 或</span><br><span class="line">cmdline.<span class="keyword">execute</span>([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'qsbk'</span>])</span><br></pre></td></tr></table></figure>



</li>
</ol>
<p><strong>qsbk.py</strong>（爬虫文件）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myspider.items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpider</span><span class="params">(scrapy.Spider)</span>:</span> <span class="comment"># 继承Spider类</span></span><br><span class="line">    name = <span class="string">'qsbk'</span> <span class="comment"># 爬虫名称，运行时查找对应的爬虫文件</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>] <span class="comment"># 限制爬虫的爬取范围</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.qiushibaike.com/text/page/1/'</span>] <span class="comment">#最开始爬取的url</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#type(response) = &lt;class 'scrapy.http.response.html.HtmlResponse'&gt;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = response.xpath(<span class="string">'//div[@class="col1 old-style-col1"]/div'</span>)<span class="comment">#提取到所有段子的div</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> result:</span><br><span class="line">            author = data.xpath(<span class="string">'.//h2/text()'</span>).get().strip()<span class="comment">#作者</span></span><br><span class="line">            content = data.xpath(<span class="string">'.//div[@class="content"]/span/text()'</span>).extract()<span class="comment">#段子内容</span></span><br><span class="line">            content = <span class="string">""</span>.join(content).strip()<span class="comment">#列表转化为字符串</span></span><br><span class="line">            item = MyspiderItem(author=author, content=content)<span class="comment">#给模型赋值，可以约定传送给pipeline的内容</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#将段子交给pipeline来进行存储</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">            <span class="comment">#yield将函数转换成生成器，通过引擎传给pipeline，需要遍历时可以从pipeline通过引擎传递回来</span></span><br><span class="line">        next_url = response.xpath(<span class="string">'//ul[@class="pagination"]/li[last()]/a/@href'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> next_url:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(<span class="string">'https://www.qiushibaike.com'</span> + next_url,callback=self.parse)<span class="comment">#callback表示请求完成后需要进行的操作，继续执行parse方法</span></span><br><span class="line">        <span class="string">'''函数不能使用return，因为使用return后会直接结束函数，不会对接下来的页面进行爬取'''</span></span><br></pre></td></tr></table></figure>

<p><strong>items.py</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="attr">import</span> <span class="string">scrapy</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">class</span> <span class="string">MyspiderItem(scrapy.Item):</span></span><br><span class="line"><span class="comment">    # define the fields for your item here like:</span></span><br><span class="line"><span class="comment">    # name = scrapy.Field()</span></span><br><span class="line"><span class="comment">    #创建数据模型，作者，内容</span></span><br><span class="line">    <span class="attr">author</span> = <span class="string">scrapy.Field()</span></span><br><span class="line">    <span class="attr">content</span> = <span class="string">scrapy.Field()</span></span><br></pre></td></tr></table></figure>

<p><strong>pipelines.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import json</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># class MyspiderPipeline(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         self.fp = open("duanzi.json", "w", encoding="utf-8")</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法</span></span><br><span class="line"><span class="comment">#         print("爬虫开始......")</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def process_item(self, item, spider):</span></span><br><span class="line"><span class="comment">#         item_json = json.dumps(dict(item), ensure_ascii=False)#先item（MyspiderItem类）转换为字典类型，再用json.dumps将item（字典类型）转换为json字符串,ensure_ascii=False表示保存中文</span></span><br><span class="line"><span class="comment">#         self.fp.write(item_json + '\n')#将数据写入文件</span></span><br><span class="line"><span class="comment">#         return item</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def close_spider(self, spider):</span></span><br><span class="line"><span class="comment">#         self.fp.close()#关闭爬虫</span></span><br><span class="line"><span class="comment">#         print("爬虫结束")</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">要想pipeline可以自动运行，需要在settings.py中去掉注释</span></span><br><span class="line"><span class="string">#ITEM_PIPELINES = &#123;</span></span><br><span class="line"><span class="string">#    'myspider.pipelines.MyspiderPipeline': 300,</span></span><br><span class="line"><span class="string">#&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">代码优化,不使用json模块，直接导入</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter,JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># class MyspiderPipeline(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         self.fp = open("duanzi.json", "wb")#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入</span></span><br><span class="line"><span class="comment">#         self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')</span></span><br><span class="line"><span class="comment">#         self.exporter.start_exporting()#开始导入</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法</span></span><br><span class="line"><span class="comment">#         print("爬虫开始......")</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def process_item(self, item, spider):</span></span><br><span class="line"><span class="comment">#         self.exporter.export_item(item)#导入item</span></span><br><span class="line"><span class="comment">#         return item</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def close_spider(self, spider):</span></span><br><span class="line"><span class="comment">#         self.exporter.finish_exporting()</span></span><br><span class="line"><span class="comment">#         self.fp.close()#关闭爬虫</span></span><br><span class="line"><span class="comment">#         print("爬虫结束")</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">使用JsonItemExplorter的缺点就是耗内存，JsonItemExplorter的工作方式是先用exporter.export_item(item)将item转换成字典然后先存放到</span></span><br><span class="line"><span class="string">JsonItemExplorter这个类中，最后exporter.finish_exporting()一并写入文件，可以改进使用JsonLinesItemExporter，JsonLinesItemExporter</span></span><br><span class="line"><span class="string">是导入一个存一个，消耗内存比较小，整个内容是一个json字符串，类型是字典列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面使用JsonLinesItemExporter这个类</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">"duanzi.json"</span>, <span class="string">"wb"</span>)<span class="comment">#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入</span></span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=<span class="literal">False</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span><span class="comment">#爬虫打开时执行，运行爬虫之前先执行此方法</span></span><br><span class="line">        print(<span class="string">"爬虫开始......"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)<span class="comment">#导入item</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()<span class="comment">#关闭爬虫</span></span><br><span class="line">        print(<span class="string">"爬虫结束"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''整个内容不再是一个json字符串，每个字典做一行'''</span></span><br></pre></td></tr></table></figure>

<p><strong>settings.py</strong>（部分）</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Scrapy settings for myspider project</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="meta"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#     https://docs.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="meta">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="meta">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'myspider.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'myspider.spiders'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = False</span><br><span class="line"></span><br><span class="line"><span class="meta"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="meta"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="meta"># See also autothrottle settings and docs</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">1</span> #设置延时<span class="number">1</span>秒</span><br><span class="line"><span class="meta"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="meta">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"><span class="meta">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">   <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># Configure item pipelines</span></span><br><span class="line"><span class="meta"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'myspider.pipelines.MyspiderPipeline'</span>: <span class="number">300</span>,#<span class="number">300</span>表示pipeline的优先级，值越小，优先级越高，可以修改</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>运行结果</strong></p>
<p><img src="/Scrapy%E6%A1%86%E6%9E%B6/6.png" alt="6"></p>
<hr>
<p><strong>基础知识总结</strong></p>
<ol>
<li><p>response是一个<code>scrapy.http.response.html.HtmlResponse</code>对象，可以直接用<code>xpath</code>和<code>css</code>语法来提取数据，不需要引用和格式化；</p>
</li>
<li><p>提取出来的数据是一个<code>Selector</code>或者<code>SelectorList</code>对象；如果想要获取其中的文本内容，可以使用<code>getall()</code>，<code>get()</code>或者<code>extract()</code>方法；</p>
</li>
<li><p><code>getall()</code>方法，获取<code>Selector</code>中的所有文本，返回的是一个列表；</p>
</li>
<li><p><code>get()</code>方法，获取<code>Selector</code>中的第一个文本，返回的是一个str类型；</p>
</li>
<li><p>如果数据解析回来，要传给pipline处理，那么可以使用<code>yield</code>来返回；或者是收集所有的item，最后统一使用return返回；</p>
</li>
<li><p>item：建议在<code>items.py</code>中定义好模型，以后就不要使用字典yield；</p>
</li>
<li><p>pipeline：专门用来保存数据，其中有三个方法将常用到：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span><span class="comment"># 当爬虫被打开的时候执行</span></span><br><span class="line">	pass</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span><span class="comment"># 当爬虫有数据（item传过来的时候被调用）</span></span><br><span class="line">	pass</span><br><span class="line">     </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span>：<span class="comment"># 当爬虫关闭的时候会被调用</span></span><br><span class="line">	pass</span><br></pre></td></tr></table></figure>

<p>要激活pipeline，应该在<code>settings.py</code>中设置<code>ITEM_PIPELINES</code>。</p>
</li>
</ol>
<p><strong>JsonItemExporter和JsonLinesItemExporter</strong></p>
<blockquote>
<p>保存json数据的时候，可以使用这两个类，让操作变得更简洁。</p>
</blockquote>
<ol>
<li><p><code>JsonItemExporter</code>：这个每次是先把数据添加到内存中，最后再统一写入磁盘，好处是存储的数据是一个json字符串，满足json规则；坏处是当数据量较大时会比较耗内存；</p>
<p>示例：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exporters import JsonItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.fp = open(<span class="string">"duanzi.json"</span>, <span class="string">"wb"</span>)<span class="comment">#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入</span></span><br><span class="line">        <span class="keyword">self</span>.exporter = JsonItemExporter(<span class="keyword">self</span>.fp, ensure_ascii=False, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        <span class="keyword">self</span>.exporter.start_exporting()<span class="comment">#开始导入</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span><span class="comment">#爬虫打开时执行，运行爬虫之前先执行此方法</span></span><br><span class="line">        print(<span class="string">"爬虫开始......"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.exporter.export_item(item)<span class="comment">#导入item</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.exporter.finish_exporting()</span><br><span class="line">        <span class="keyword">self</span>.fp.close()<span class="comment">#关闭爬虫</span></span><br><span class="line">        print(<span class="string">"爬虫结束"</span>)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="2">
<li><p><code>JsonLinesItemExporter</code>：这个是每次调用<code>export_item</code>的时候就把这个item存储到硬盘中；好处是每次处理数据的时候就直接存储到了硬盘中，不会占太多内存，数据也比较安全，坏处是每个字典（数据）就是一行，整个文件不满足json文件格式，需要用json.loads()方法来转换为json格式；</p>
<p>示例：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exporters import JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.fp = open(<span class="string">"duanzi.json"</span>, <span class="string">"wb"</span>)<span class="comment">#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入</span></span><br><span class="line">        <span class="keyword">self</span>.exporter = JsonLinesItemExporter(<span class="keyword">self</span>.fp, ensure_ascii=False, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span><span class="comment">#爬虫打开时执行，运行爬虫之前先执行此方法</span></span><br><span class="line">        print(<span class="string">"爬虫开始......"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.exporter.export_item(item)<span class="comment">#导入item</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.fp.close()<span class="comment">#关闭爬虫</span></span><br><span class="line">        print(<span class="string">"爬虫结束"</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><div class="sharethis-inline-share-buttons"></div><script src="/" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zhifubao.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/weixin.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">文件上传</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/python%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B5/"><span class="level-item">python爬虫实践</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="" src="/img/avatar.png" alt="WebDog"></figure><p class="title is-size-4 is-block line-height-inherit">WebDog</p><p class="is-size-6 is-block">莫得感情的Web渗透测试机器</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ChengDu</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/webysx" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/webysx"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/php%E4%BB%A3%E7%A0%81%E5%AE%A1%E8%AE%A1/"><span class="level-start"><span class="level-item">php代码审计</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/python%E7%88%AC%E8%99%AB/"><span class="level-start"><span class="level-item">python爬虫</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/sql%E6%B3%A8%E5%85%A5/"><span class="level-start"><span class="level-item">sql注入</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"><span class="level-start"><span class="level-item">文件上传</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/php%E4%BB%A3%E7%A0%81%E5%AE%A1%E8%AE%A1/"><span class="tag">php代码审计</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python%E7%88%AC%E8%99%AB/"><span class="tag">python爬虫</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sql%E6%B3%A8%E5%85%A5/"><span class="tag">sql注入</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"><span class="tag">文件上传</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-30T12:42:14.000Z">2020-03-30</time></p><p class="title is-6"><a class="link-muted" href="/kali%E5%AE%89%E8%A3%85/">kali安装</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-28T01:58:28.000Z">2020-03-28</time></p><p class="title is-6"><a class="link-muted" href="/%E9%AA%8C%E8%AF%81%E7%A0%81%E5%AE%89%E5%85%A8/">验证码安全</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-25T11:07:25.000Z">2020-03-25</time></p><p class="title is-6"><a class="link-muted" href="/ctf/">ctf</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-25T10:10:07.000Z">2020-03-25</time></p><p class="title is-6"><a class="link-muted" href="/xxe%E6%BC%8F%E6%B4%9E/">xxe漏洞</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-25T10:09:15.000Z">2020-03-25</time></p><p class="title is-6"><a class="link-muted" href="/%E9%80%BB%E8%BE%91%E6%BC%8F%E6%B4%9E/">逻辑漏洞</a></p><p class="is-uppercase"></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="WebDog" height="28"></a><p class="size-small"><span>&copy; 2020 WebDog</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://yoursite.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>